\documentclass{article}

\title{ECE 863 - Homework \#1}
\author{Sam Boling}
\date{\today}

\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\newcommand{\horline}
           {\begin{center}
              \noindent\rule{8cm}{0.4pt}
            \end{center}}

\begin{document}

\maketitle

\section*{Problem 8.1}
A language identification system at an international airport is trained
to recognize six languages:

\begin{center}
  \{ Hindi, Arabic, Spanish, Korean, Mandarin, French \}
\end{center}

\begin{enumerate}[label=(\alph*)]
\item{What is the sample space, $\mathcal{S}$, for an "experiment" in which
      the system attempts to recognize the language of a random speaker?
      \horline      
      Assuming the system will identify only one language to each speaker,
      the sample space is the set of all languages the system might
      identify. This is the set \{ Hindi, Arabic, Spanish, Korean, 
      Mandarin, French \}.
     }
\item{Which of the following terms accurately characterize $\mathcal{S}$?
      $\bullet$ discrete, $\bullet$ continuous, $\bullet$ mixed, 
      $\bullet$ countable, $\bullet$ finite, $\bullet$ uncountable, 
      $\bullet$ countably infinite, $\bullet$ Gaussian.
      \horline
      The sample space is discrete, countable, and finite.
     }
\item{What is the "natural" event space, $\mathcal{A}$, for this problem
      (do not attempt to provide an exhaustive list). How many elements
      are in the set $\mathcal{A}$?
      \horline
      A straightforward choice of event space is the power set 
      $\mathbb{P}(\mathcal{S})$, the set of all subsets of $\mathcal{S}$.
      Its cardinality is $2^{|\mathcal{S}|} = 2^6 = 64$.
     }
\item{What is the probability mass distribution that should be assigned to
      the sample points in $\mathcal{S}$? In answering, let 
      $\{\zeta_1, \zeta_2, \dots, \zeta_6\}$ correspond to the six languages
      in the order listed above (e.g., ($\zeta_1$ = Hindi)), and let $p[i]$
      denote $P(\zeta_1)$.
      \horline
      If speakers of each language are equally likely to be found
      at the airport, a good choice of probability mass function would be
      $p(\zeta_i) = \frac{1}{6}, i = 1, \dots, 6$. More generally, a 
      reasonable choice would be $p(\zeta_i) = \frac{m_i}{N}$, where $m_i$
      is the number of speakers of the language in the airport and $N$ is 
      the total number of people in the airport.
     }
\item{Suppose we are told only that $\{\zeta_1\} \in \mathcal{A}$. What 
      other event(s) must be in $\mathcal{A}$? Which events in 
      $2^{|\mathcal{S}|}$ need \textit{not} be in $\mathcal{A}$?
      \horline
      By Property 2 of an event space, $\zeta_1 \in A$ implies
      $\zeta_1^c = \{\zeta_2, \zeta_3, \dots, \zeta_6\} \in A$.  Aside from
      $\mathcal{S} = \zeta_1 \cup \zeta_1^c$, no other events need to 
      belong to the event space, i.e. $\zeta_2$ through $\zeta_6$ need not
      individually be in $A$.
     }
\end{enumerate}

\section*{Problem 8.2}
Suppose that the language identification described in Problem 8.1 is
upgraded to identify the gender of the speaker as well as the language
spoken. Assume that the system is used randomly and equally often by men
and women, as well as randomly and equally often by speakers of all six
languages. Here we consider an "experiment" in which the system recognizes
the language and gender of a random speaker. The sample space consists of
the sample points
$$\mathcal{S} = \zeta_{m1}, \zeta_{f1}, \zeta_{m2}, \dots, \zeta_{f6}$$
in which subscripts "$m$" and "$f$" denote "male" and "female," and the
accompanying integer indexes one of the six languages in the order listed
in Problem 8.1

\begin{enumerate}[label=(\alph*)]
\item{In formal terms, what are the events
      \begin{center}
        $A$ = female speaker, and $B$ = speaker of a national language in
        Europe
      \end{center}
      \begin{enumerate}[label=(\roman*)]
        \item{Formally show that events $A$ and $B$ are statistically
              independent.
             }
        \item{Compute the probability $P(A \cup B)$.
             }
      \end{enumerate}
      \horline
      $A = \{ \zeta_{f1}, \zeta_{f2}, \dots, \zeta_{f6} \}$.
      $B = \{ \zeta_{f2}, \zeta_{m2}, \zeta_{f6}, \zeta_{m6} \}$.
      \begin{enumerate}[label=(\roman*)]
        \item{Since each event is equally likely and since 
              $|\mathcal{S}| = 12$, we have
              $$P(\zeta_{ij}) = \frac{1}{12}, i = m,f, j = 1, \dots, 6.$$
              Then 
              \begin{align*}
              P(A) & = P\left(\bigcup_{i=1}^6 \zeta_{fi} \right) \\
                   & = \sum_{i=1}^6 P(\zeta_{fi})  = \frac{6}{12} 
                                                   = \frac{1}{2}
              \end{align*}
              and similarly $P(B) = \frac{4}{12} = \frac{1}{3}$.
              $A \cup B = \{\zeta_{f2}, \zeta_{f6}\},$ so 
              $$P(A \cup B) = \frac{2}{12} = \frac{1}{6} = P(A)P(B)$$.
              Therefore $A$ and $B$ are statistically independent.
             }
      \end{enumerate}
     }
\item{Formally show that the events $A$ and $B$ described in part (b) are
      \textit{not} mutually exclusive.
      \horline
       $A \cap B = \{ \zeta_{f2}, \zeta_{f6} \} \neq \varnothing$, so
       $A$ and $B$ are not mutually exclusive.
     }
\end{enumerate}

\section*{Problem 8.8}
The concepts of mutual exclusivity and statistical independence are often
confused. They are, however, very different concepts describing very
dissimilar relationships between events in a probability space.
\begin{enumerate}[label=(\alph*)]
  \item{\begin{enumerate}[label=(\roman*)]
          \item{Write a simple non-technical describtion of what we mean, in
                the context of a random experiment, when we say that events
                $A$ and $B$ are "mutually exclusive".
               }
          \item{Write a similar statement describing the "independence" of
                events $A$ and $B$.
               }
          \item{If $A$ and $B$ are mutually exclusive in the sense you have
                described in part (i), is their relationship consistent
                with the requirements of "independence" you have
                described in part (ii)? Might it be said that to be
                "mutually exclusive," $A$ and $B$ must exhibit a strong
                "dependence" upon one another?
               }
        \end{enumerate}
        \horline
        \begin{enumerate}[label=(\roman*)]
          \item{Events $A$ and $B$ are mutually exclusive if $A$ contains
                none of the outcomes in $B$. 
               }
          \item{Events $A$ and $B$ are independent if their outcomes do not
                depend on each other -- the likelihood of choosing an 
                outcome that is in both event $A$ and event $B$ is the 
                product of their individual likelihoods.
               }
        \end{enumerate}
       }
  \item{Now give an analytical argument showing that $A$ and $B$ are 
        mutually exclusive of one another \textit{and} statistically
        independent if and only if at least one of the events has
        probability zero.
        \horline
        \begin{itemize}
          \item[($\Rightarrow$)]
          {
            Let $A$ and $B$ be mutually exclusive and statistically 
            independent. Since they are mutually exclusive, 
            $A \cap B = \varnothing$, so $P(A \cap B) = P(\varnothing) = 0$. 
            Since they are independent, $P(A \cap B) = P(A)P(B)$. But if 
            $P(A)P(B) = 0$ then either $P(A) = 0$ or $P(A) = 0$. Therefore,
            if $A$ and $B$ are both mutually exclusive and independent, then
            $P(A) = 0$ or $P(B) = 0$.
          }
          \item[($\Leftarrow$)]
          { Not necessarily true! As a counterexample, let $A = \{x\}$ and
            $B = (x-\frac{1}{2}, x+\frac{1}{2}) = \mathcal{S}$ for some 
            $x \in \mathbb{R}$. 
            If we choose as a probability measure
            $$
            P(U) = \int_{\zeta \in U} d\zeta
            $$
            then $P(A) = 0$, but since $A \cap B = \{ x \} = A$, the events
            are not mutually exclusive. 
          }
        \end{itemize}
       }
\end{enumerate}

\section*{Problem 8.10}
In Sec. 8.4, a \textit{symmetric binary channel} is defined as a 
transmission channel for which the conditional probability that the channel
will invert the transmitted bit does not depend on the value of the bit
sent; that is $P(Y_{1-i} | X_i)$ does not depend on $i$.
\begin{enumerate}[label=(\alph*)]
  \item{Argue that the symmetric channel is equivalently defined as one for
        which the conditional probability that the channel will preserve
        the transmitted bit does not depend on the value of the bit sent;
        that is $P(Y_i | X_i)$ does not depend on $i$.
        \horline
        Since $\{Y_0, Y_1\}$ forms a partition of the sample space, we have
        that 
        $$
        X_i = (Y_0 \cap X_i) \cup (Y_1 \cap X_i)
        $$
        so
        \begin{align*}
        P(X_i) & = P(Y_0 \cap X_i) + P(Y_1 \cap X_i) - P(Y_0 \cap _Y1 \cap X_i)\\
               & = P(Y_0 \cap X_i) + P(Y_1 \cap X_i)
        \end{align*}
        since $Y_0 \cap Y_1 \cap X_i = \varnothing$. Then
        \begin{align*}
        1 & = \frac{P(Y_0 \cap X_i)}{X_i} + \frac{P(Y_1) \cap X_i}{X_i}\\
          & = P(Y_0 | X_i) + P(Y_1 | X_i)
        \end{align*}
        so
        $$
        P(Y_0 | X_i) = 1 - P(Y_1 | X_i)
        $$
        which means that $P(Y_0 | X_0) = 1 - P(Y_1 | X_0)$, and
        $$
        P(Y_1 | X_i) = 1 - P(Y_0 | X_i)
        $$
        which means that $P(Y_1 | X_1) = 1 - P(Y_0 | X_1)$, so
        $$
        P(Y_i | X_i) = 1 - P(Y_{1-i} | X_i),
        $$
        so $P(Y_i | X_i)$ does not depend on $i$.
       }
  \item{Show that the probability that a bit is correctly transmitted is
        independent of the prior probabilities in a symmetric binary 
        channel.
        \horline
        The probability of a correct transmission is given by 
        \begin{align*}
        P(Y_0 \cap X_0) + P(Y_1 \cap X_1) 
          & = P(Y_0 | X_0) P(X_0) + P(Y_1 | X_1) P(X_1),
        \end{align*}
        but from part (a), $P(Y_0 | X_0) = P(Y_1 | X_1)$. Thus
        $$
        P(Y_0 \cap X_0) + P(Y_1 \cap X_1) = P(Y_0 | X_0)(P(X_0) + P(X_1)),
        $$
        but since $\{X_0, X_1\}$ is a partition of the sample space we have
        $P(X_0) + P(X_1) = 1$. Thus the probability of a correct 
        transmission is $P(Y_0 | X_0)$.
       }
  \item{Show that the probability that a bit is not correctly transmitted
        is independent of the prior probabilities in a symmetric binary
        channel.
        \horline
        The probability of an incorrect transmission is given by
        \begin{align*}
        P(Y_0 \cap X_1) + P(Y_1 \cap X_0) 
          & = P(Y_0 | X_1) P(X_1) + P(Y_1 | X_0) P(X_0),
        \end{align*}
        but from the definition of the channel 
        $P(Y_0 | X_1) = P(Y_1 | X_0)$. Thus
        $$
        P(Y_0 \cap X_1) + P(Y_1 \cap X_0) = P(Y_0 | X_1) (P(X_1) + P(X_0)),
        $$
        but since $\{X_0, X_1\}$ is a partition of the sample space we have
        $P(X_0) + P(X_1) = 1$. Thus the probability of an incorrect
        transmission is $P(Y_0 | X_1)$.
       }
\end{enumerate}
\end{document}

