\documentclass{article}

\title{ECE 863 - Homework \#6}
\author{Sam Boling}
\date{\today}

\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\newcommand{\horline}
           {\begin{center}
              \noindent\rule{8cm}{0.4pt}
            \end{center}}

\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}

\begin{document}

\maketitle

\section*{Problem \#10.13}
If $V[n_1] = V_0$, then
$$
r_{\nu}[n_1, n_2] = c_{\nu}[n_1,n_2] - \mu_{\nu}[n_1]\mu_{\nu}[n_2] = V_0 \delta[n_1 - n_2], 
$$
which depends only on the time difference $n_1-n_2$. Since the mean is constant, this means that the process is wide-sense stationary.

\section*{Problem \#10.14}
We see that
\begin{align*}
\Gamma_{\underline{x}}(f) &= \sum_{-\infty}^{\infty}r_{\underline{x}}[\eta] e^{-2\pi f j \eta} 
                           = \sum_{-N_0}^{N_0}e^{-2 \pi f j \eta} \\
                          &= \sum_{-N_0}^{N_0}(e^{-2\pi f j})^{\eta},
\end{align*}
which is a geometric series so
\begin{align*}
\Gamma_{\underline{x}}(f) &= \frac{e^{2 \pi f j N_0} - e^{-2 \pi f j (N_0 + 1)}}{1 - e^{- 2 \pi f j}}
                           = \frac{e^{\pi f j}(e^{2 \pi f j N_0} - e^{-2 \pi f j (N_0 + 1)}}{e^{\pi f j} - e^{-\pi f j}} \\
                          &= \frac{e^{\pi f j(2 N_0 + 1)} - e^{\pi f j(-2 N_0 - 2 + 1)}}{e^{\pi f j} - e^{-\pi f j}} 
                           = \frac{e^{\pi f j(2 N_0 + 1)} - e^{-\pi f j(2N_0 + 1)}}{e^{\pi f j} - e^{- \pi f j}} \\
                          &= \frac{\sin (\pi f [2 N_0 + 1])}{\sin (\pi f)}.
\end{align*}

\section*{Problem \#10.15}
We have
\begin{align*}
\Gamma_{\underline{y}}(f) &= |H(f)|^2 \Gamma_{\underline{\nu}}(f) \\
\Gamma_{\underline{y}_I}(f) &= |H_I(f)|^2 \Gamma_{\underline{\nu}}(f)
\end{align*}
and since $\Gamma_{\underline{\nu}}(f) = V_0$ this means
\begin{align*}
\Upsilon_{\underline{y}_I} 
  &= 2\int_{0}^{0.5} \Gamma_{\underline{w}}(f) ~df 
   = 2\int_{0}^{b} H_0^2 V_0 ~df \\
  &= 2V_0 H_0^2 b
\end{align*}
and
\begin{align*}
\Upsilon_{\underline{y}}(f)
  &= 2\int_{0}^{0.5} |H(f)|^2 V_0 ~df.
\end{align*}
Setting these average powers equal we see
$$
2V_0 H_0^2 b = 2V_0 \int_{0}^{0.5} |H(f)|^2 ~df
$$
so
$$
b = \frac{1}{H_0^2} \int_{0}^{0.5} |H(f)|^2 ~df.
$$

\section*{Problem \#10.16}
We see the average power for the ideal bandpass filter is
\begin{align*}
\Upsilon_{\underline{y}_I} 
  &= 2\int_{f_c - \frac{b}{2}}^{f_c + \frac{b}{2}} G^2 V_0 ~df
          = 2V_0G^2 \left[f_c + \frac{b}{2} 
                        - \left(f_c - \frac{b}{2}\right)\right] \\
         &= 2V_0 G^2 b
\end{align*}
so
$$
b = \frac{1}{G^2} \int_{0}^{0.5} |H(f)|^2 ~df.
$$

\section*{Problem \#10.17}
We see that
\begin{align*}
r_{\underline{x}}[n_1, n_2] 
  &= \mathcal{E}\left\{\underline{x}[n_1]\underline{x}[n_2]\right\}
   = \mathcal{E}\left\{\cos(2 \pi f_0 n_1 + \underline{\theta})
                       \cos(2 \pi f_0 n_2 + \underline{\theta})\right\} \\
  &= \frac{1}{2}\mathcal{E}\left\{
       \cos(2 \pi f_0 n_1 + \underline{\theta} 
        - [2 \pi f_0 n_2 + \underline{\theta}])
     + \cos(2 \pi f_0 n_1 + \underline{\theta}
        + [2 \pi f_0 n_2 + \underline{\theta}])\right\} \\
  &= \frac{1}{2}\mathcal{E}\left\{
       \cos(2 \pi f_0 [n_1 - n_2]) 
     + \cos(2 \pi f_0 [n_1 + n_2] + 2\underline{\theta})\right\} \\
  &= \frac{1}{2} \cos (2 \pi f_0 [n_1 - n_2])
   + \frac{1}{2} \mathcal{E}\left\{
       \cos (2 \pi f_0 [n_1 + n_2] + 2\underline{\theta})\right\}.
\end{align*}
But since $\underline{\theta}$ is uniformly distributed over $[0, 2\pi)$,
\begin{align*}
\mathcal{E}\left\{\cos(2 \pi f_0 [n_1 + n_2] + 2\underline{\theta})\right\}
 &= \int_{0}^{2\pi} \frac{1}{2 \pi} 
      \cos(2 \pi f_0 [n_1 + n_2] + 2\theta) ~d\theta = 0,
\end{align*}
since this is an integral over whole periods of a sinusoid. Therefore
$$
r_{\underline{x}}[n_1, n_2] = \frac{1}{2} \cos (2 \pi f_0 [n_1 - n_2])
$$
or
$$
r_{\underline{x}}[\eta] = \frac{1}{2} \cos (2 \pi f_0 \eta).
$$
Thus the autocorrelation has the same period in $\eta$ as $\underline{x}$
has in $n$.

Then the PDS is
\begin{equation*}
\begin{split}
\Gamma_{\underline{x}}(f) 
  &= \sum_{\eta = -\infty}^{\infty} r[\eta] e^{-2 \pi j f \eta} 
   = \frac{1}{2} \sum_{\eta = -\infty}^{\infty} 
       \cos(2 \pi f_0 \eta) e^{-2\pi j f \eta} \\
  &= \frac{1}{2} \left\{1 
   + \sum_{\eta = 1}^{\infty}\cos(2 \pi f_0 \eta) e^{-2 \pi j \eta f}
   + \sum_{\eta = 1}^{\infty}\cos(2 \pi f_0 \eta) e^{2 \pi j \eta f}
     \right\} \\
  &= \frac{1}{2}\left\{1
   + \frac{1}{2}\sum_{\eta=1}^{\infty}\left[
       e^{-2 \pi j \eta (f - f_0)} + e^{2 \pi j \eta (f - f_0)}
     + e^{2 \pi j \eta (f + f_0)} + e^{-2 \pi j \eta (f + f_0)}\
                                      \right]\right\} \\
  &= \frac{1}{2}\left\{1
   + \sum_{\eta=1}^{\infty}
       \cos(2 \pi \eta [f-f_0]) + \cos(2 \pi \eta [f + f_0])\right\}.
\end{split}
\end{equation*}
If $f = \pm f_0$, then the summand reduces to 1 so 
$\Gamma_{\underline{x}}(f_0)$ is infinite. Otherwise, the geometric series
of the exponential forms above is
\begin{align*}
  \sum_{\eta=1}^{\infty}&\left[
     e^{-2 \pi j \eta (f - f_0)} + e^{2 \pi j \eta (f - f_0)}
   + e^{2 \pi j \eta (f + f_0)} + e^{-2 \pi j \eta (f + f_0)}\
                                      \right] \\
&= \frac{e^{-2 \pi j \eta (f-f_0)}}{1 - e^{-2 \pi j \eta (f-f_0)}}
 + \frac{e^{2 \pi j \eta(f-f_0)}}{1 - e^{2 \pi j \eta (f-f_0)}}
 + \frac{e^{2 \pi j \eta(f+f_0)}}{1 - e^{2 \pi j \eta (f+f_0)}}
 + \frac{e^{-2\pi j \eta(f+f_0)}}{1 - e^{-2\pi j \eta (f+f_0)}} \\
&= \frac{e^{-\pi j \eta (f-f_0)} - e^{\pi j \eta (f-f_0)}}
        {e^{\pi j \eta (f-f_0)} - e^{-\pi j \eta (f-f_0)}}
 + \frac{e^{-\pi j \eta (f+f_0)} - e^{\pi j \eta (f+f_0)}}
        {e^{\pi j \eta (f+f_0)} - e^{-\pi j \eta (f+f_0)}} = -2,
\end{align*}
so $\Gamma_{\underline{x}}(f) = 0$ for $f \neq \pm f_0$, so
all the power is concentrated at $\pm f_0$.

\section*{Problem \#10.20}
Consider the random processes
$$
\underline{x}[n] = \cos (2 \pi f_0 + \underline{\theta}), 
\underline{y}[n] = \sin (2 \pi f_0 + \underline{\theta}),
$$
where $\underline{\theta}$ is uniformly distributed on $[0, 2\pi)$. 
As shown in the previous problem, this means
$$
r_{\underline{x}}[\eta] = \frac{1}{2} \cos (2 \pi f_0 \eta),
$$
and similarly
\begin{align*}
r_{\underline{y}}[n_1, n_2] &= \mathcal{E}\left\{
  \sin(2 \pi f_0 n_1 + \underline{\theta})\sin(2 \pi f_0 n_2 + \underline{\theta})\right\} \\
  &= \frac{1}{2} \mathcal{E}\left\{
    \cos (2 \pi f_0 [n_1 - n_2]) - \cos (2 \pi f_0 [n_1 + n_2] + 2\underline{\theta})\right\} \\
  &= \cos (2 \pi f_0 [n_1 - n_2]),
\end{align*}
so 
$$
\Upsilon_{\underline{x}} = r_{\underline{x}}[0] = \frac{1}{2}
 = r_{\underline{y}}[0] = \Upsilon_{\underline{y}}
$$
and
\begin{align*}
  r_{\underline{x}\underline{y}}[\eta] &= \mathcal{E}\left\{
    \cos (2 \pi f n_1 + \underline{\theta})
    \sin (2 \pi f n_2 + \underline{\theta})\right\} \\
  &= \frac{1}{2} \mathcal{E} \left\{
       \sin (2 \pi f (n_1 + n_2) + 2 \underline{\theta}) +
       \sin (2 \pi f (n_1 - n_2)\right\} \\
  &= \frac{1}{2} \sin (2 \pi f \eta),
\end{align*}
so the processes are correlated.
Thus the cross PDS is
\begin{align*}
  \Gamma_{\underline{x} \underline{y}} (f) &=
    \frac{1}{2} \sum_{\eta = -\infty}^{\infty}
      \sin (2 \pi f_0 \eta) e^{- 2 \pi j f \eta} \\
  &= \frac{1}{2} \left\{
       \sum_{\eta = 1}^{\infty} \sin (2 \pi f_0 \eta)
                                e^{-2 \pi j f \eta} 
     - \sum_{\eta = 1}^{\infty} \sin (2 \pi f_0 \eta)
                                e^{2 \pi j f \eta} \right\}\\
  &= -j \sum_{\eta = 1}^{\infty} \sin (2 \pi f_0 \eta)
                                 \sin (2 \pi f \eta) \\
  &= \frac{j}{2} \sum_{\eta = 1}^{\infty}\left\{
       \cos (2 \pi \eta [f - f_0]) 
     - \cos (2 \pi \eta [f + f_0]) \right\}\\
  &= \frac{j}{2} [ \delta(f - f_0) + \delta (f + f_0) ],
\end{align*}
so $\Upsilon_{\underline{x} \underline{y}} = j$.
Compare this to the correlation between white noise
processes whose individual powers sum to 1.


\section*{Problem \#10.22}
Let $\underline{\mathbf{z}}$ be the joint Gaussian process with random variables
taken from mutually uncorrelated Gaussian processes $\underline{\mathbf{x}}$ and 
$\underline{\mathbf{y}}$.
Then
\begin{align*}
\mathbf{C}_{\underline{\mathbf{z}}} &= \mathcal{E}\left\{\left[\begin{array}{c}
  \underline{x}[n_1] - \mu_{\underline{x}}[n_1] \\
                   \vdots                       \\
  \underline{x}[n_N] - \mu_{\underline{x}}[n_N] \\
  \underline{y}[m_1] - \mu_{\underline{y}}[m_1] \\
                    \vdots                      \\
  \underline{y}[m_M] - \mu_{\underline{y}}[m_M] 
\end{array}\right]
\left[\begin{array}{c}
  \underline{x}[n_1] - \mu_{\underline{x}}[n_1] \\
                   \vdots                       \\
  \underline{x}[n_N] - \mu_{\underline{x}}[n_N] \\
  \underline{y}[m_1] - \mu_{\underline{y}}[m_1] \\
                    \vdots                      \\
  \underline{y}[m_M] - \mu_{\underline{y}}[m_M] 
\end{array}\right]^T\right\} \\ 
&= \mathcal{E}\left\{
\left[
\scalemath{0.5}{
\begin{array}{c c c c c c}
(\underline{x}[n_1] - \mu_{\underline{x}}[n_1])^2 & 
\hdots & 
(\underline{x}[n_1] - \mu_{\underline{x}}[n_1])(\underline{x}[n_N] - \mu_{\underline{x}}[n_N]) &
0 & \hdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
(\underline{x}[n_N] - \mu_{\underline{x}}[n_N])(\underline{x}[n_1] - \mu_{\underline{x}}[n_1]) &
\hdots &
(\underline{x}[n_N] - \mu_{\underline{x}}[n_N])^2 &
0 & \hdots & 0 \\
0 & \hdots & 0 & 
(\underline{y}[m_1] - \mu_{\underline{y}}[m_1])^2 & 
\hdots &
(\underline{y}[m_1] - \mu_{\underline{y}}[m_1])(\underline{y}[m_M] - \mu_{\underline{y}}[m_1]) \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \hdots & 0 &
(\underline{y}[m_M] - \mu_{\underline{y}}[m_M])(\underline{y}[m_1] - \mu_{\underline{y}}[m_1]) &
\hdots &
(\underline{y}[m_M] - \mu_{\underline{y}}[m_M])^2
\end{array}
}
\right]\right\} \\
&=
\left[
\begin{array}{c c c c}
\mathbf{C}_{\underline{\mathbf{x}}} & \mathbf{0}_{N \times M} \\
\mathbf{0}_{M \times N}    & \mathbf{C}_{\underline{\mathbf{y}}}
\end{array}
\right].
\end{align*}

Then
$$
\det \mathbf{C}_{\underline{\mathbf{z}}} 
  = (\det \mathbf{C}_{\underline{\mathbf{x}}})
    (\det \mathbf{C}_{\underline{\mathbf{y}}})
$$
and
$$
\mathbf{C}_{\underline{\mathbf{z}}}^{-1} =
\left[\begin{array}{c c}
\mathbf{C}_{\underline{\mathbf{x}}}^{-1} &
  \mathbf{0}_{N \times M} \\
\mathbf{0}_{M \times N} &
  \mathbf{C}_{\underline{\mathbf{y}}}^{-1}
\end{array}\right],
$$
so
\begin{align*}
(\mathbf{z} - \mu_{\underline{\mathbf{z}}})^T 
  \mathbf{C}_{\underline{\mathbf{z}}}^{-1}
  (\mathbf{z} - \mu_{\underline{\mathbf{z}}}) 
&=(\mathbf{x} - \mu_{\underline{\mathbf{x}}})^T 
  \mathbf{C}_{\underline{\mathbf{x}}}^{-1}
  (\mathbf{x} - \mu_{\underline{\mathbf{x}}}) +
  (\mathbf{y} - \mu_{\underline{\mathbf{y}}})^T 
  \mathbf{C}_{\underline{\mathbf{y}}}^{-1}
  (\mathbf{y} - \mu_{\underline{\mathbf{y}}}) 
\end{align*}
Then
\begin{align*}
f_{\underline{\mathbf{z}}}(\mathbf{z})
&= \frac{1}{(2\pi)^{\frac{N+M}{2}}
  \sqrt{\det \mathbf{C}_{\underline{\mathbf{x}}}}
  \sqrt{\det \mathbf{C}_{\underline{\mathbf{y}}}}
} \\
& \hphantom{= =}
\left[\exp\left\{
  (\mathbf{x} - \mu_{\underline{\mathbf{x}}})^T 
  \mathbf{C}_{\underline{\mathbf{x}}}^{-1}
  (\mathbf{x} - \mu_{\underline{\mathbf{x}}}\right\}
      \exp\left\{ 
  (\mathbf{y} - \mu_{\underline{\mathbf{y}}})^T 
  \mathbf{C}_{\underline{\mathbf{y}}}^{-1}
  (\mathbf{y} - \mu_{\underline{\mathbf{y}}}\right\}
  \right]^{-1/2} \\
&= f_{\underline{\mathbf{x}}}(\mathbf{x})
   f_{\underline{\mathbf{y}}}(\mathbf{y}),
\end{align*}
so the process $\underline{\mathbf{z}}$ is independent.


\section*{Problem \#10.23}
We see that
$$
\mu_{\underline{x}}[n] = \mathcal{E}\left\{\underline{x}[n]\right\} 
 = \int_{-\infty}^{\infty} x f_{\underline{x}[n]}(x) ~dx,
$$
and since $\underline{x}$ is i.i.d., $f_{\underline{x}[n]}(x) = 
f_{\underline{x}}(x)$ is independent of $n$, so $\mu_{\underline{x}}$
is constant.

We see next that if $n_1 = n_2 = n$,
\begin{align*}
c_{\underline{x}}[n_1, n_2] &= \mathcal{E}\left\{
  (\underline{x}[n] - \mu_{\underline{x}})^2
  \right\} \\
&= \mathcal{E}\left\{(\underline{x}[n])^2\right\}
 - 2\mu_{\underline{x}} \mathcal{E}\left\{\underline{x}[n]\right\}
 + \mu_{\underline{x}}^2 \\
&= \mathcal{E}\left\{(\underline{x}[n])^2\right\} - \mu_{\underline{x}}^2,
\end{align*}
which is constant since $\underline{x}$ is i.i.d. and therefore
$f_{\underline{x}[n]}$ does not depend on $n$.
Otherwise,
\begin{align*}
c_{\underline{x}}[n_1, n_2] &= \mathcal{E}\left\{
  (\underline{x}[n_1] - \mu_{\underline{x}})
  (\underline{x}[n_2] - \mu_{\underline{x}})\right\} \\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
     (x_1 - \mu_{\underline{x}})(x_2 - \mu_{\underline{x}}) 
     f_{\underline{x}[n_1]\underline{x}[n_2]}(x_1, x_2) 
     ~\mathrm{d}x_1 
     ~\mathrm{d}x_2 \\
&= \int_{-\infty}^{\infty} 
    (x_1 - \mu_{\underline{x}})f_{\underline{x}}(x_1)
  \left[
    \int_{-\infty}^{\infty}
      (x_2 - \mu_{\underline{x}})f_{\underline{x}}(x_2)
     ~\mathrm{d}x_2
  \right]
     ~\mathrm{d}x_1 \\ 
&= \int_{-\infty}^{\infty} 
    (x_1 - \mu_{\underline{x}})f_{\underline{x}}(x_1)
  \left[
    \int_{-\infty}^{\infty}x_2 f_{\underline{x}}(x_2) 
      ~\mathrm{d}x_2
  - \mu_{\underline{x}}\int_{-\infty}^{\infty} f_{\underline{x}}(x_2)
      ~\mathrm{d}x_2
  \right]
    ~\mathrm{d}x_1,
\end{align*}
but 
$$
\int_{-\infty}^{\infty}x_2 f_{\underline{x}}(x_2) ~\mathrm{d}x_2 =
 \mu_{\underline{x}}
$$
and
$$
\int_{-\infty}^{\infty} f_{\underline{x}}(x_2) ~\mathrm{d}x_2 = 1,
$$
so $c_{\underline{x}}[n_1, n_2] = 0$ for $n_1 \neq n_2$. Therefore
$$
c_{\underline{x}}[n_1, n_2] = \sigma_{\underline{x}}^2 \delta[n_1 - n_2]
$$
where the constant $\sigma_{\underline{x}}^2$ is given by
$$
\sigma_{\underline{x}}^2 = \mathcal{E}\left\{(\underline{x}[n])^2\right\} - \mu_{\underline{x}}^2
$$
as above. It follows that 
$$
r_{\underline{x}}[n_1, n_2] = c_{\underline{x}}[n_1, n_2] 
                            + \mu_{\underline{x}}[n_1]\mu_{\underline{x}}[n_2]
                            = \sigma_{\underline{x}}^2 \delta[n_1 - n_2] + \mu_{\underline{x}}^2.
$$

The parameter $\sigma_{\underline{x}}^2$ gives the mean-centered linear dependence of a random
variable $\underline{x}[n]$ with itself.

These results mean that the statistical mean $\mu_{\underline{x}}$
is constant and $r_{\underline{x}}[n_1, n_2]$ depends only on the spacing $n_1 - n_2$, so an
i.i.d. process is wide-sense stationary. Furthermore such a process is first-order stationary,
since the fact that it is identically distributed gives
$$
f_{\underline{x}[n]} = f_{\underline{x}[n + \Delta]} = f_{\underline{x}}.
$$
Since the process is independent,
\begin{align*}
f_{\underline{x}[n_1 + \Delta] \cdots \underline{x}[n_N + \Delta]}
  (x_1, \dots, x_N) 
&= f_{\underline{x}[n_1 + \Delta]}(x_1) \cdots
   f_{\underline{x}[n_N + \Delta]}(x_N) \\
&= f_{\underline{x}[n_1]}(x_1) \cdots
   f_{\underline{x}[n_N]}(x_N) \\
&= f_{\underline{x}[n_1] \cdots \underline{x}[n_N]}
  (x_1, \dots, x_N).
\end{align*}
Thus an i.i.d. process is strict-sense stationary.


\end{document}
