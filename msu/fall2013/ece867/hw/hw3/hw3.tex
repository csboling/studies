\documentclass{article}

\title{ECE 867 - Homework \#3}
\author{Sam Boling}
\date{\today}

\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\newcommand{\horline}
           {\begin{center}
              \noindent\rule{8cm}{0.4pt}
            \end{center}}

\begin{document}

\maketitle

A three state time-invariant Markov chain $\{X_{k}\}$ is characterized by
the following homogeneous transition matrix:
$$
P = \left[\begin{array}{c c c}
  \overline{\alpha} & \alpha & 0 \\
  \beta             & 0      & \overline{\beta} \\
  0                 & \gamma & \overline{\gamma}
\end{array}\right]
$$
where $\overline{x} = 1 - x$.
\begin{enumerate}[label=(\alph*)]
  \item{Assume that the process starts with an initial state distribution
        that is uniform:
        $$
        \pi^{(1)} = \left[\begin{array}{c c c}
            \frac{1}{3} & \frac{1}{3} & \frac{1}{3}
                    \end{array}\right].
        $$
        What is the entropy $H(X_2)$ of the process at time index $k=2$?
        \horline
        We see that
        \begin{align*}
        \pi^{(2)} & = \pi^{(1)}P = \frac{1}{3}\left[\begin{array}{c c c}
          \overline{\alpha} + \beta & 
          \alpha + \gamma           &
          \overline{\beta} + \overline{\gamma}\end{array}\right]
        \end{align*}
        so
        \begin{align*}
        H(X_2) &= -\frac{1}{3}\left(
        (\overline{\alpha} + \beta)\log \frac{\overline{\alpha} + \beta}{3}
      + (\alpha + \gamma)\log \frac{\alpha + \gamma}{3}
      + (\overline{\beta} + \overline{\gamma})
          \log\frac{\overline{\beta} + \overline{\gamma}}{3}\right)
        \end{align*}
       }
  \item{Find the stationary state probability distribution $\pi$:
        $$
        \pi = \left[\pi_1 \pi_2 \pi_3\right].
        $$
        \horline
        Solving $\pi = \pi P$, we see
        \begin{align*}
        \pi_1 &= \frac{\beta}{\alpha} \pi_2, \\
        \pi_2 &= \alpha \pi_1 + \gamma \pi_3 \\
        \pi_3 &= \frac{\overline{\beta}}{\gamma} \pi_2.
        \end{align*}
        The second equation is redundant, but since 
         $\pi_1 + \pi_2 + \pi_3 = 1$ we have
        \begin{align*}
        \pi_2 &= \frac{1}{1 + \frac{\beta}{\alpha} 
                 + \frac{\overline{\beta}}{\alpha}}\\
        \pi_1 &= \frac{\beta}{\alpha + \beta 
                 + \frac{\alpha\overline{\beta}}{\gamma}}\\
        \pi_3 &= \frac{\overline{\beta}}{\gamma + \overline{\beta}
                 + \frac{\beta \gamma}{\alpha}}.
        \end{align*}
       }
  \item{Find the stationary marginal entropy $H(X)$ of the process.
       \horline
       From the stationary distribution given above,
       \begin{align*}
       H(X) &= -\left(\pi_1 \log \left(\pi_2 \frac{\beta}{\alpha}\right)
         + \pi_2 \log \pi_2 
         + \pi_3 \log \left(\pi_2 \frac{\overline{\beta}}{\gamma}\right)
               \right) \\
            &= -\pi_1 \log \frac{\beta}{\alpha} 
               -\pi_2 \log \frac{\overline{\beta}}{\gamma}
               -(\pi_1 + \pi_2 + \pi_3)\log \pi_2 \\
            &= -\left(\log \pi_2 + \pi_1\log\frac{\beta}{\alpha}
                                 + \pi_2\log\frac{\overline{\beta}}{\gamma}
                \right).
       \end{align*}
       }
  \item{Find the entropy rate $H_X$.
       \horline
       Since $X$ is a stationary Markov process,
       \begin{align*}
         H_X &= -\sum_{ij} \mu_i P_{ij} \log P_{ij} \log P_{ij} \\
             &= 
       \end{align*}
       }
\end{enumerate}

\end{document}

