\documentclass{article}

\title{ECE 867 - Homework \#1}
\author{Sam Boling}
\date{\today}

\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\newcommand{\horline}
           {\begin{center}
              \noindent\rule{8cm}{0.4pt}
            \end{center}}

\begin{document}

\maketitle

\section*{Problem 2.1}
Let $X$ and $Y$ be random variables with alphabets $\mathcal{X} = \mathcal{Y} = \{1,2,3,4,5\}$ and joint distribution $p(x,y)$ given by
$$
\frac{1}{25}
\left[
\begin{array}{c c c c c}
  1 & 1 & 1 & 1 & 1 \\
  2 & 1 & 2 & 0 & 0 \\
  2 & 0 & 1 & 1 & 1 \\
  0 & 3 & 0 & 2 & 0 \\
  0 & 0 & 1 & 1 & 3 
\end{array}\right].
$$
Determine $H(X), H(Y), H(X|Y), H(Y|X)$, and $I(X;Y)$.
\horline
The marginal distributions are 
$$
p(X) = \left(\frac{5}{25}, \frac{5}{25}, \frac{5}{25}, 
             \frac{5}{25}, \frac{5}{25}\right),
p(Y) = \left(\frac{5}{25}, \frac{5}{25}, \frac{5}{25},
             \frac{5}{25}, \frac{5}{25}\right)
$$
or $p_X(k) = p_Y(k) = \frac{1}{5} \forall k$.
The marginal entropies are therefore
\begin{align*}
H(X) = H(Y) & = -\sum_{i=1}^{5} p(i) \log p(i)\\ 
            & = -\log \frac{1}{5} = \log 5 \approx 2.3219 \mbox{bits}
\end{align*}
while
\begin{align*}
H(X|Y) & = -\sum_{y=1}^5 \sum_{x=1}^5 p(y, x) \log p(x|y)\\
       & = -\sum_{y=1}^5 \sum_{x=1}^5 p(y, x) \log \frac{p(y, x)}{p_Y(y)}\\
       & = \frac{1}{25}(-5(1 \log 5 \frac{1}{25}) \\
       &  -(2 \log 5 \frac{2}{25} 
           + 1 \log 5 \frac{1}{25} 
           + 2 \log 2 \frac{2}{25}) \\
       &  -(2 \log 5 \frac{2}{25} 
           + 1 \log 5 \frac{1}{25} 
           + 1 \log 5 \frac{1}{25}) \\
       &  -(3 \log 5 \frac{3}{25} 
           + 2 \log 5 \frac{2}{25}) \\
       &  -(1 \log 5 \frac{1}{25} 
           + 1 \log 5 \frac{1}{25} 
           + 3 \log 5 \frac{3}{25})) \\
       & = -\frac{2}{5} \log \frac{1}{5} 
           - \frac{8}{25} \log \frac{2}{5} - \frac{6}{25} \log \frac{3}{5}
       & \approx 1.5287 \mbox{bits}.
\end{align*}
Since $H(Y|X) = H(X) - H(Y) + H(X|Y)$, $H(Y|X) \approx 1.5287$ bits as 
well.

\section*{Problem 2.4}
Verify that $p(x,y,z)$ as defined in Definition 2.4 is a probability
distribution. You should exclude all the zero probability masses from the 
summation carefully.
\horline
We wish to show that
$$
\sum_{x,y,z} p(x,y,z) = \sum_{x \in \mathcal{X}} 
                        \sum_{y \in \{y | p(y) > 0\}} 
                        \sum_{z \in \mathcal{Z}} \frac{p(x,y)p(y,z)}{p(y)}
 = 1.
$$
\begin{align*}
\sum_{x,y,z}p(x,y,z) 
  &=\sum_{S_y} \frac{1}{p(y)} \sum_{z} p(y,z) \sum_{x} p(x,y)
\end{align*}

\section*{Problem 2.7}
Prove that $H(p)$ is concave in $p$, i.e., for $0 \leq \lambda \leq 1$ and
$\overline{\lambda} = 1 - \lambda$,
$$
\lambda H(p_1) + \overline{\lambda} H(p_2) 
  \leq H(\lambda p_1 + \overline{\lambda} p_2).
$$
\horline
The entropy $H(p)$ is given by
$$
H(p) = -p \log p - (1 - p) \log (1 - p)
$$
which has the derivative
\begin {align*}
H\prime(p) & = -\log p - 1 - (-\log(1-p) + (1 - p)(-\frac{1}{1-p}) \\
           & = -\log p + \log(1 - p)
\end{align*}
and thus the second derivative
$$
H^{\prime\prime}(p) = -\frac{1}{p} - \frac{1}{1 - p}.
$$

\section*{Problem 2.8}
Let $(X,Y) \sim p(x,y) = p(x)p(y|x)$.
\begin{enumerate}[label=\alph*)]
  \item{Prove that for fixed $p(x), I(X;Y)$ is a convex functional of 
        $p(y|x)$.
        \horline
        We have that
        \begin{align*}
        I(X;Y) &= \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)} 
                = D(p(x,y) \| p(x)p(y))
        \end{align*}
        or alternatively
        \begin{align*}
         I(X ; Y) &= \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)} 
                   = \sum_{x,y} p(x)p(y|x) \log \frac{p(y|x)}{p(y)}\\
        \end{align*}
        so
        \begin{align*}
        I(\lambda p_1(y|x) + \overline{\lambda}p_2(y|x)) &=
        \sum_{x,y} (\lambda p_1(y|x) 
                    + \overline{\lambda}p_2(y|x))p(x) 
                   \log \frac{\lambda p_1(y|x) + \overline{\lambda}p_2(y|x)}
                             {p(y)} \\
          & \leq 
        \end{align*}
       }
  \item{Prove that for fixed $p(y|x), I(X;Y)$ is a concave functional of
        $p(x)$.
       }
\end{enumerate}
\horline
\begin{enumerate}[label=\alph*)]
  \item{We have that
       \begin{align*}
         I(X ; Y) &= \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
                   = \sum_{x,y} p(x) p(y|x) \log \frac{p(y|x)}{p(y)} \\
       \end{align*}
       so for fixed $p(x)$ and denoting $p = p(y|x)$ we may write $I$ as
       $$
       I(p, p(y)) = \sum_{x,y} p(x) p \log \frac{p}{p(y)} 
       $$
       so that
       \begin{align*}
       \lambda I(p_1, p(y)) + (1 - \lambda) I(p_2, p(y)) & = 
           \lambda \sum_{x,y} p(x) p_1 \log \frac{p_1}{p(y)} 
         + (1 - \lambda) \sum_{x,y} p(x) p_2 \log \frac{p_2}{p(y)} \\
        & \geq \lambda \sum_x p(x) p_1 \log p_1 
             + (1 - \lambda) \sum_x p(x) p_2 \log p_2
       \end{align*}
       by the log-sum inequality, since $\sum_y p(y) = 1$. But
       \begin{align*}
       I(\lambda p_1 + (1 - \lambda) p_2, p(y)) & = 
         (\lambda p_1 + (1-\lambda)p_2) \sum_{x,y} p(x) 
            \log \frac{\lambda p_1 + (1-\lambda)p_2}{p(y)} \\
        &\geq \sum_{x} p(x) (\lambda p_1+(1-\lambda)p_2) 
             \log \frac{\lambda p_1+(1-\lambda)p_2}{\sum_y p(y)} \\
        &=\sum_x p(x)(\lambda p_1+(1-\lambda)p_2)
          \log(\lambda p_1+(1-\lambda)p_2).
       \end{align*}
       }
  \item{
       }
\end{enumerate}

\section*{Problem 2.9}
Do $I(X;Y) = 0$ and $I(X; Y|Z) = 0$ imply each other? If so, give a proof.
If not, give a counterexample.
\horline
Suppose that $I(X;Y|Z) = 0$ and $I(X;Z) = I(Y;Z) = I(X;Y)$.
Then we have
\begin{align*}
H(X) - H(X|Z) &= H(Y) - H(Y|Z) &= H(X) - H(X|Y) \\
H(X) - H(X|Z) + H(Y|Z) &= H(Y) &= H(X) - H(X|Y) + H(Y|Z) \\
H(X) + H(Y|Z) &= H(Y) + H(X|Z) &= H(X) - H(X|Y) + H(X|Z) + H(Y|Z).
\end{align*}
But since $I(X;Y|Z) = 0$, $H(X|Z) + H(Y|Z) = H(X,Y|Z)$, so
$$
H(X) - H(X|Y) + H(X,Y|Z) = I(X;Y) + H(X, Y|Z) = H(X) + H(Y|Z)
$$
which gives
\begin{align*}
I(X; Y) &= H(X) + H(Y|Z) - H(X,Y|Z) = H(X) + H(Y|Z) - (H(X,Y,Z)-H(X,Y))\\
        &= H(X)+H(Y|Z) - ((H(X)+H(X|Y)+H(X|Y,Z))-(H(X)+H(X|Y))) \\ 
        &= H(X)+H(Y|Z) - H(X|Y,Z),
\end{align*}
and since conditioning does not increase entropy, $H(X|Y,Z) \leq H(X)$. 
Therefore $I(X;Y) \geq 0$, demonstrating a counterexample to the reverse
implication.

\section*{Problem 2.11}
Let $X$ be a function of $Y$. Prove that $H(X) \leq H(Y)$. Interpret this
result.
\horline 

This result means that a deterministic function on a random variable does
not add information. It can only collapse the distribution, not broaden it.

\section*{Problem 2.12}
Prove that for any $n \geq 2$,
$$
H(X_1, X_2, \cdots, X_n) \geq \sum_{i=1}^{n} H(X_i | X_j, j \neq i).
$$
\horline
\begin{enumerate}
  \item{$H(X_1, X_2) = H(X_1) + H(X_2 | X_1)$, but 
        $H(X_1) \geq H(X_1 | X_2)$, so 
        $H(X_1, X_2) \geq \sum_{i=1}^2 H(X_i | X_j)$ where $i \neq j$.}
  \item{Suppose for some $k$ that 
        $H(X_1, \dots, X_k) \geq \sum_{i=1}^k H(X_i | X_j)$, where
        $i \neq j$. Then from the chain rule for entropy,
        $$
        H(X_1, \dots, X_{k+1}) - H(X_1, \dots, X_k)
         = H(X_{k+1} | X_k, \dots, X_1),         
        $$
        so 
        \begin{align*}
        H(X_1, \dots, X_{k+1}) 
          & =    H(X_1, \dots, X_k) + H(X_{k+1} | X_k, \dots, X_1) \\
          & \geq \sum_{i=1}^k H(X_i | X_j, j \neq i) 
                  + H(X_{k+1} | X_k, \dots, X_1)
        \end{align*}
        by inductive hypothesis. But
        $$
        \sum_{i=1}^k H(X_i | X_j, j \neq i) + H(X_{k+1} | X_k, \dots, X_1) 
        = \sum_{i=1}^{k+1} H(X_i | X_j, j \neq i),
        $$
        concluding the proof.
        }
\end{enumerate}

\section*{Problem 2.10}
Give an example for which $D(\dot \| \dot)$ does not satisfy the triangular
inequality.
\horline
We wish to find $p$, $q$ and $r$ such that
$$
D(p \| r) > D(p \| q) + D(q \| r).
$$
Let $\mathcal{X} = \{1, 2, 3\}$ and let
$$
p(x) = \left\{ \begin{array}{l l}
                 1, & x = 1 \\
                 0, & \mbox{otherwise}
               \end{array} \right. ,
q(x) = \left\{ \begin{array}{l l}
                 \frac{1}{3}, & x \in \{1, 2, 3\} \\
                 0,           & \mbox{otherwise}
               \end{array} \right. ,
r(x) = \left\{ \begin{array}{l l}
                 \frac{1}{4}, & x \in \{1, 3\} \\
                 \frac{1}{2}, & x = 2 \\
                 0,           & \mbox{otherwise}
               \end{array} \right. .
$$
Then
\begin{align*}
D(p \| q) & = \sum_x p(x) \log \frac{p(x)}{q(x)} 
          & = 1 \log \frac{1}{\frac{1}{3}} = \log 3 \approx 1.585, \\
D(q \| r) & = \frac{1}{3} \log \frac{\frac{1}{3}}{\frac{1}{4}}
            + \frac{1}{3} \log \frac{\frac{1}{3}}{\frac{1}{2}}
            + \frac{1}{3} \log \frac{\frac{1}{3}}{\frac{1}{4}} 
          & = \frac{2}{3} \log \frac{4}{3} + \frac{1}{3} \log \frac{2}{3}
            \approx 0.0817,
\end{align*}
so $D(p \| q) + D(q \| r) \approx \frac{5}{3}$, but
\begin{align*}
D(p \| r) & = 1 \log \frac{1}{\frac{1}{4}} = \log 4 = 2.
\end{align*}

\section*{Problem 2.15}
Prove the divergence inequality using the log-sum inequality.
\horline
See 2.103-2.105 in Cover \& Thomas.

\section*{Problem 2.16}
Prove that $D(p \| q)$ is convex in the pair $(p,q)$, i.e., if $(p_1, q_1)$
and $(p_2, q_2)$ are two pairs of probability distributions on a common
alphabet, then
$$
D(\lambda p_1 + \overline{\lambda} p_2 \| \lambda q_1 + \overline{\lambda}q_2)
  \leq \lambda D(p_1 \| q_1) + \overline{\lambda} D(p_2 \| q_2)
$$
for all $0 \leq \lambda \leq 1$, where $\overline{\lambda} = 1 - \lambda$.
\horline
See Thm. 2.7.2 in Cover \& Thomas.

\section*{Problem 2.17}
Let $p_{XY}$ and $q_{XY}$ be two probability distributions on 
$\mathcal{X} \times \mathcal{Y}$. Prove that 
$D(p_{XY} \| q_{XY}) \geq D(p_X \| q_X)$.
\horline
We have that
\begin{align*}
  D(p_{XY} \| q_{XY}) & = \sum_{x,y} p_{XY}(x, y) \log p_{XY}(x, y)
                      & \geq \sum_x \left[\sum_y p_{XY}(x, y)\right]
                      \log \frac{\sum_y p_{XY}(x,y)}{\sum_y q_{XY}(x,y)} \\
                      & = \sum_x p_X(x) \log \frac{p_X(x)}{q_X(x)}
\end{align*}
by the log-sum inequality, which is what we set out to prove.

\end{document}

