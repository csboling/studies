\documentclass{report}
\usepackage{fullpage}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{commath}

\usepackage{enumerate}
\usepackage{mathtools}

\author{Sam Boling}
\title{
  Review Notes \\
  ME/ECE 851 \\
  Linear Control Theory \\
  Fall 2014 \\
  with Ranjan Mukherji \\
  Text: A Linear Systems Primer, Antsaklis \& Michel
}

\begin{document}

\begin{titlepage}
\maketitle
\end{titlepage}

\section{Preliminaries}

\subsection{Matrix Exponential}
The matrix exponential is given by
$$
e^{A} = \sum_{k=0}^\infty \frac{A^k}{k!}
$$
which can be computed by the inverse Laplace transform
$$
e^{At} = \mathcal{L}^{-1}\{(sI - A)^{-1}\}
$$
or in some special cases:
\begin{align*}
\left[\begin{array}{c c c c}
  \lambda_1 & 0          & \cdots & 0      \\
  0         & \lambda_2  & \cdots & 0      \\
  \vdots    & \vdots     & \ddots & \vdots \\
  0         & 0          & \cdots & \lambda_n
\end{array}\right]
& \mapsto
\left[\begin{array}{c c c c}
  e^{\lambda_1} & 0           & \cdots & 0      \\
  0           & e^{\lambda_2} & \cdots & 0      \\
  \vdots      & \vdots      & \ddots & \vdots \\
  0           & 0           & \cdots & e^{\lambda_n}
\end{array}\right] \\
\left[\begin{array}{c c c c c}
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 1 \\
  0 & 0 & 0 & 0
\end{array}\right]
& \mapsto
\left[\begin{array}{c c c c}
  1 & t & \frac{t^2}{2} & \frac{t^3}{3!} \\
  0 & 1 & t             & \frac{t^2}{2}  \\
  0 & 0 & 1             & t              \\
  0 & 0 & 0             & 1
\end{array}\right].
\end{align*}

Note that $e^{A} e^{B} = e^{A + B}$ if any only if $A$ and $B$ commute.

\subsection{Cayley-Hamilton Theorem}
The Cayley-Hamilton theorem states that every matrix satisfies its own
characteristic polynomial. That is, let
$$
\det (sI - A) = \alpha(s) = \sum_{k=0}^{n} \alpha_k s^k.
$$
Then $\sum_{k=0}^{n} \alpha_k A^k = 0$. A consequence of this is
that
$$
A^n = \sum_{k=0}^{n-1} \beta_k A^k,
$$
so $A^n$ can be written as a linear combination of lesser powers of
$A$ and therefore
$$
\mathrm{rank}
\left[\begin{array}{c}
  I      \\
  A      \\
  A^2    \\
  \cdots \\
  A^n    \\
  A^{n+1} \\
  \cdots \\
  A^{n+k}
\end{array}\right]
=
\mathrm{rank}
\left[\begin{array}{c}
  I      \\
  A      \\
  A^2    \\
  \cdots \\
  A^n
\end{array}\right]
$$
for any $k \geq 1$.

\subsection{Diagonalizability, Jordan Normal Form}
If the sum of the eigenspaces of an $n \times n$ matrix $A$ is equal
to $n$, then the matrix is diagonalizable. This is equivalent to the
condition that the matrix has $n$ linearly independent eigenvectors
(i.e. the geometric multiplicity of each eigenvalue is equal to its
arithmetic multiplicity)
and indeed a diagonalizing transformation is given by
$\hat{A} = P^{-1} A P$ where $P^{-1}$ is a matrix composed of said
eigenvectors. In this case the diagonal matrix has the eigenvalues of
$A$ along the diagonal.

For each eigenspace with dimension less than the multiplicity of the
associated eigenvalue, we can find a generalized eigenvector by
solving $(A - \lambda I)v^\prime = v$, where $v$ is a true
eigenvector. Equivalently, generalized eigenvectors are solutions to
the equation $(A - \lambda I)^k v = 0$ for some $k \geq 1$. The Jordan
normal form then consists of Jordan blocks, each with the associated
eigenvalue $\lambda_i$ repeated $p_i$ times (where $p_i$ is its arithmetic
multiplicity) and ones in the superdiagonal.

This process is an example of a similarity transformation,
i.e. finding a matrix $P$ to transform $A$ to a desired form by the
conjugation $\hat{A} = P^{-1} A P$. This can be considered as a change
of variables for the state equation, which results in a new system of
equations given by the matrices
$$
\hat{A} = P^{-1} A P, \quad
\hat{B} = P^{-1} B, \quad
\hat{C} = C P, \quad
\hat{D} = D.
$$

\section{Linear Time-Invariant Systems}
We are interested in linear dynamical systems of the form
$$
\dot{x} = A(t) x(t) + B(t) u(t), \quad
y = C(t) x(t) + D(t) u(t),
$$
typically with constant matrices $A, B, C, D$ under the assumption
that the system dynamics are time-invariant. Similarly, discrete
linear time-invariant systems have the form
$$
x(k+1) = A x(k) + B u(k), \quad
y(k) = C x(k) + D u(k).
$$
It is evident that a discrete system has the solution
$$
x(k) = A^k x(0) + \sum_{n=0}^{k-1} A^{k-n-1} B u(n)
$$
and a successive-approximation argument will inductively show that the
continuous system has the solution
$$
x(t) = e^{At} x(0) + \int_0^t e^{A(t - \tau)} B u(\tau) \dif \tau,
$$
and the Laplace-domain transfer function relating $y(t)$ to $u(t)$ is then
$$
H(s) = C(sI - A)^{-1} B,
$$
from which the identity $e^{At} = \mathcal{L}^{-1}\{(sI - A)^{-1}\}$
is evident. Note that a similarity transformation on the state model
($\hat{A} = P^{-1} A P$), etc. does not affect the transfer function of
the system.

We can also consider a sampled-data system, which is a discrete-time
system described by the matrices
\begin{align*}
\bar{A}(k) &= \phi(t_{k+1}, t_k), \\
\bar{B}(k) &=
  \int_{t_k}^{t_{k+1}} \phi(t_{k+1}, \tau) B(\tau) \dif\tau, \\
\bar{C}(k) &= C(t_k^\prime)\phi(t_k^\prime, t_k), \\
\bar{D}(k) &=
  C(t_k^\prime)
  \int_{t_k}^{t_k^\prime} \phi(t_k^\prime, \tau) B(\tau) \dif \tau
+ D(t_k^\prime
\end{align*}
when the output sampling times are given by $t_k^\prime$ and the input
is modified at times $t_k$.

\subsection{Equilibrium Points and Stability}
A dynamical system
$$
\dot{x} = f(x)
$$
has equilibrium points wherever $f(x) = 0$. If $f(x)$ is continuously
differentiable with respect to each of its arguments then $x_e$ is an
equilibrium whenever
$$
\left[\frac{\partial f_i}{\partial x_j}\right]_{x = x_e} = 0.
$$

We can linearize a system
$$
\dot{x}(t) = f(t, x, u), \quad
y(t) = h(t, x, u)
$$
about known solutions $\bar{x}(t)$,
$\bar{u}(t)$ by considering a small perturbation $u_\delta(t)$ and the
corresponding $x_\delta(t)$, i.e.
$x(t) = \bar{x}(t) + x_\delta(t)$. This linearization is given by
$$
\dot{x}_\delta(t) =
\left[\frac{\partial f}{\partial x}\right]
  _{\substack{x = \bar{x}(t) \\ u = \bar{u}(t)}}
x_\delta(t)
+
\left[\frac{\partial f}{\partial u}\right]
  _{\substack{x = \bar{x}(t) \\ u = \bar{u}(t)}}
u_\delta(t).
$$

We can notice from the dependence of the solutions on
the state transition matrix $e^{At}$ that, in the linear
time-invariant case, the eigenvalues of $A$
completely determine the stability of the system. In the
continuous-time case we require that
$\mathcal{Re}\{\lambda_i\} < 0$ for each eigenvalue $\lambda_i$. In
the discrete-time case it is instead necessary that
$|lambda_i| < 1$ for the system to be stable.

\section{Controllability and Observability Tests}

We are interested in questions of controllability and
observability. Here are some definitions:
\begin{itemize}
  \item{
    A system is called \emph{reachable} if there exists an input that
    drives the system states from the origin to an arbitrary $x(t_f)$
    in finite time $t_f$. In continuous LTI systems this is
    equivalent to controllability. In discrete LTI systems this
    implies controllability.
  }
  \item{
    A system is \emph{controllable} if there exists an input that
    drives the system states from any initial $x(t_0)$ to the origin
    in finite time. In discrete LTI system this implies reachability
    only if the matrix $A$ is nonsingular.
  }
  \item{
    A system is called \emph{observable} if it is possible to
    determine its present state from current and future outputs $y(t)$
    and inputs $u(t)$ of the system. In continuous LTI systems this is
    equivalent to constructibility. In discrete LTI systems this
    implies constructibility.
  }
  \item{
    A system is called \emph{constructible} if it is possible to
    determine its present state from knowledge of current and past
    outputs and inputs. In discrete LTI system this implies
    observability only if the matrix $A$ is nonsingular.
  }
  \item{
    A system is called \emph{stabilizable} if its uncontrollable
    eigenvalues are stable.
  }
  \item{
    A system is called \emph{detectable} if its unobservable
    eigenvalues are stable.
  }
\end{itemize}

The following are equivalent conditions for
controllability/reachability of continuous time systems.
\begin{itemize}
  \item{
    $\mathrm{rank} W_C(0, T) = n$ for some finite $T > 0$,
    where the controllability Gramian $W_C$ is given by
    $$
    W_C(0, T) = \int_0^T e^{A \tau} B B^\top e^{-A^\top \tau} \dif \tau.
    $$
    Note also that
    $$
    W_r(0, T)
    = e^{A T} W_C(0, T) e^{A^\top T}
    = \int_0^T e^{(T - \tau)A} B B^\top e^{(T - \tau)A^\top} \dif \tau
    $$
    and for a reachable system, the control input producing a state $x_f$
    from a state $x_0$ in time $T$ is
    $$
    u(t) =
    B^\top e^{A^\top(T - t)} W_r^{-1}(0, T) [x_f - e^{A T} x_0].
    $$
  }
  \item{
    The rows of $e^{A t} B$ (or of $(sI - A)^{-1} B$) are linearly
    inddependent over the field of complex numbers.
  }
  \item{
    The controllability matrix
    $$
    \mathcal{C} = [B, A B, \cdots, A^{n-1} B]
    $$
    has rank $n$.
  }
  \item{
   The following condition is met: if $v_i [ \lambda_i I - A, B ] = 0$
   for any eigenvalue $\lambda_i$, then $v_i = 0$. (i.e., this matrix
   has no nontrivial left eigenvectors).
  }
  \item{
    $\mathrm{rank} [ \lambda_i I - A, B ] = n$ for each eigenvalue $\lambda_i$
    of $A$. The eigenvalues yielding a rank less than $n$ are the
    uncontrollable eigenvalues of the system.
  }
\end{itemize}

The following are conditions for the observability of the pair
$(A,C)$.
\begin{itemize}
  \item{
    The observability Gramian $W_o(0, T)$ has rank $n$ for some $T$,
    in which case
    $$
    x_0 =
    W_O^{-1}(0, T)\int_0^T e^{A^\top \tau} C^\top \tilde{y}(\tau) \dif
    \tau,
    $$
    where
    $$
    W_0(0, T) = \int_0^T e^{A^\top \tau} C^\top C e^{A \tau} \dif \tau
    $$
    and
    $$
    \tilde{y}(t)
    = y(t)
    - \left[
        \int_0^T C e^{A(t - \tau)} B u(\tau) \dif \tau + D u(t)
      \right]
    $$
  }
  \item{
    $\mathrm{rank}(\mathcal{O}) = n$, where
    $$
    \mathcal{O} =
    \left[\begin{array}{c}
      C        \\
      C A      \\
      \vdots   \\
      C A^{n-1}
    \end{array}\right].
    $$
  }
  \item{
    The columns of $C e^{A t}$ or $C (sI - A)^{-1}$ are linearly independent over the field
    of complex numbers.
  }
  \item{
    $\mathrm{rank}
       \left[\begin{array}{c}
         \lambda_i I - A \\
         B
       \end{array}\right] = n$ for each eigenvalue $\lambda_i$
    of $A$. The eigenvalues yielding a rank less than $n$ are the
    unobservable eigenvalues of the system.
  }
\end{itemize}

A system is constructible if

The dual system to
$$
\dot{x} = A x + B u, \quad
y = C x + D u
$$
is given by
$$
A_D = A^\top, \quad
B_D = C^\top, \quad
C_D = B^\top, \quad
D_D = D^\top.
$$

\section{Controller and Observer Forms}

To determine the controllability (observability) of a system, first compute
the controllability (observability) matrix $\mathcal{C}$ ($\mathcal{O}$):
$$
\mathcal{C} =
\left[\begin{array}{c c c c c}
  B & A B & A^2 B & \cdots & A^{n-1} B
\end{array}\right], \quad
\mathcal{O} =
\left[\begin{array}{c}
  C        \\
  C A      \\
  C A^2    \\
  \vdots   \\
  C A^{n-1}
\end{array}\right]
$$
where $n$ is the rank of $A$. If the controllability (observability)
matrix is of full rank, the system is controllable (observable).

\subsection{Controller Form -- Single Input}
If the system is controllable, then in the single-input case
$\mathcal{C}$ is $n \times n$, so $\mathcal{C}^{-1}$ exists. Let $q$
be the $n$th row of $\mathcal{C}^{-1}$. Then the state transformation
$$
P =
\left[\begin{array}{c}
  q        \\
  q A      \\
  q A^2    \\
  \vdots   \\
  q A^{n-1}
\end{array}\right]
$$
produces the controller form
$$
A_C = P A P^{-1} =
\left[\begin{array}{r r r r}
  0         & 1         & \cdots & 0      \\
  \vdots    & \vdots    & \ddots & \vdots \\
  0         & 0         & \cdots & 1      \\
  -\alpha_0 & -\alpha_1 & \cdots & -\alpha_{n-1}
\end{array}\right], \quad
B_C = PB =
\left[\begin{array}{c}
  0      \\
  0      \\
  \vdots \\
  1
\end{array}\right], \quad
C_C = CP^{-1}, \quad
D_C = D.
$$
Note that the $\alpha_i$ above are the coefficients in the
characteristic polynomial
$$
\det (sI - A) =
s^n + \alpha_{n-1} s^{n-1} + \cdots + \alpha_1 s + \alpha_0.
$$

\subsection{Uncontrollable Form }
Let $\mathrm{rank} \mathcal{C} = r$. Take $r$ linearly independent
columns $v_i$ of $\mathcal{C}$, and choose $n - r$ vectors $w_i$ linearly
independent from these. Construct the transformation matrix
$$
Q =
\left[\begin{array}{c c c c c c}
  v_1 & \cdots & v_r & w_1 & \cdots & w_{n-r} \\
\end{array}\right].
$$
Then the uncontrollable form of the system is
$$
\hat{A} = Q^{-1} A Q =
\left[\begin{array}{c c}
  A_{11} & A_{12} \\
  0     & A_{22}
\end{array}\right], \quad
\hat{B} = Q^{-1} B =
\left[\begin{array}{c}
  B_1 \\
  0
\end{array}\right],
$$
where $(A_{11}, B_1)$ is controllable. The eigenvalues of $A_{11}$ are
controllable. Furthermore note that the transformed controllability
matrix $\hat{\mathcal{C}} = Q^{-1} \mathcal{C}$ has the controllable
subspace of the system as its range.

Note also that
$$
e^{A(t - \tau)} B
= [ Q e^{\hat{A}(t - \tau)} Q^{-1} ]
  [ Q \hat{B} ]
= Q\left[\begin{array}{c}
     e^{A_1 (t - \tau)} B_1 \\
     0
   \end{array}\right]
$$
contains only the controllable modes of the system, so the input
cannot affect the uncontrollable modes.

\subsection{Observer Form}
Suppose the system is observable.
Let $\bar{q}$ be the $n$th column of $\mathcal{O}^{-1}$ and let
$$
P^{-1} = Q = [\bar{q}, A \bar{q}, \dots, A^{n-1} \bar{q}].
$$
Then
$$
A_O =
\left[\begin{array}{r r r r}
  0      & \cdots & 0      & -\alpha_0 \\
  1      & \cdots & 0      & -\alpha_1 \\
  \vdots & \ddots & \vdots & \vdots    \\
  0      & \cdots & 1      & -\alpha_{n-1}
\end{array}\right], \quad
C_O = [0, \dots, 0, 1].
$$
where
$$
A_O = P A P^{-1}, \quad
B_O = P B, \quad
C_O = C P^{-1}, \quad
D_O = D
$$
and
$$
\det (sI - A) = s^n + \alpha_{n-1} s^{n-1} + \cdots + \alpha_1 s + \alpha_0.
$$

\subsection{Unobservable Form}
Let $r = \mathrm{rank}(\mathcal{O}) < n$. Take
$v_i$ to be a basis for the null space of $\mathcal{O}$. Then the
transformation matrix
$$
Q =
\left[\begin{array}{c c c c }
  Q_1 & v_1 & \cdots & v_{n-r}
\end{array}\right],
$$
where $Q_1$ is any matrix that makes $Q$ nonsingular, takes the system
to the form
$$
\hat{A} = Q^{-1} A Q =
\left[\begin{array}{c c}
  A_{11} & 0      \\
  A_{21} & A_{22}
\end{array}\right], \quad
\hat{C} = C Q =
\left[\begin{array}{c c}
  C_1 & 0
\end{array}\right].
$$
Furthermore the matrix $\hat{\mathcal{O}} = \mathcal{O} Q$ has a null
space equal to the unobservable subspace of $(\hat{A}, \hat{C})$.

\subsection{Kalman Decomposition}

Let $n_r = \mathrm{rank}(\mathcal{C})$,
$n_{\bar{o}} = n - \mathrm{rank}(\mathrm{O})$, and $n_{r\bar{o}}$ be the dimension
of the subspace that is controllable but unobservable. Take
$$
Q = [v_1, \dots, v_{n_r - n_{r\bar{o}} + 1}, \dots, v_{n_r}, Q_N,
     \hat{v}_1, \dots, \hat{v}_{n_{\bar{o}} - n_{r \bar{o}}}],
$$
where $v_1, \dots, v_{n_r}$ forms a basis for the controllable subspace,
$v_{n_r - n_{r\bar{o}} + 1}, \dots, v_{n_r}$ forms a basis for the
controllable and unobservable subspace, etc. % TODO

\subsection{Multi-input Systems}
For a multi-input system we must first compute the controllability
indices $\mu_i$. Compute $\mathcal{C}$ and take $n$ linearly independent
columns, rearranging these into the matrix
$$
\bar{\mathcal{C}} =
\left[
  b_1, A b_1, \dots, A^{\mu_1 - 1} b_1,
  \dots,
  b_m, \dots, A^{\mu_m - 1} b_m
\right]
$$
where $b_1, \dots, b_m$ are the $m$ columns of the input matrix $B$.
(If the $b_i$ are not all linearly independent we can choose a
linearly independent subset and reformulate the system.)
Here $\mu_i$ denotes the number of columns involving $b_i$.

Next, compute $\bar{\mathcal{C}}^{-1}$ and let $q_k$ denote the $\sigma_k$
row, where $\sigma_k = \sum_{i=1}^k \mu_i$. Then
$$
P =
\left[\begin{array}{c}
  q_1            \\
  q_1 A          \\
  \vdots         \\
  q_1 A^{\mu_1 - 1} \\
  \vdots         \\
  q_m            \\
  \vdots         \\
  q_m A^{\mu_m - 1}
\end{array}\right].
$$
Then $A_C = P A P^{-1}$, $B_C = P B$.

We note also that
$$
A_C = \bar{A}_C + \bar{B}_C A_m, \quad
B_c = \bar{B}_C + B_m
$$
where $\bar{A}_C = \mathrm{blkdiag}[\bar{A}_{11}, \dots,
\bar{A}_{mm}]$, $\bar{A}_{ii}$ is an identity matrix with $\mu_i - 1$
columns with a 0 column and row appended on the left and bottom, and
$\bar{B}_C$ is a block diagonal matrix formed from $m$ $\mu_i \times 1$
zero vectors with a 1 in the last row.

Controllability indices are invariant under similarity and input
transformations, as well as state feedback.

\subsection{Multi-Output Case}
Proceed similarly, taking linearly independent rows of $\mathcal{O}$
and finding observability indices $\nu_i$, forming $\bar{\mathcal{O}}$,
and taking $\tilde{q}_k$ to be
$\tilde{\sigma}_k$ column of $\bar{\mathcal{O}}^{-1}$.

\section{Controller Design}
For the system given by
$$
\dot{x} = Ax + Bu, \quad
y = Cx + Du,
$$
consider the control input $u = Fx + r$
so that the system becomes
$$
\dot{x} = (A + BF)x + Br, \quad
y = (C + DF) x + Dr.
$$

An open-loop (feedforward) controller can be produced by
$$
u = F[sI - (A + BF)]^{-1} x_0 + [I - F(sI - A)^{-1} B]^{-1} r,
$$
but this requires exact knowledge of the initial state $x_0$ and the
dynamics matrices.

\subsection{Eigenvalue/Eigenvector Placement}
The controllable eigenvalues of the closed loop system
$\dot{x} = (A + BF)x$ can be assigned arbitrarily by choice of
feedback matrix $F$.

For a single-input system in controller form, the feedback
$[f_0, \dots, f_{n-1}]$ gives
$$
A_C + B_C F =
\left[\begin{array}{c c c c}
   0                & 1                 & \cdots & 0      \\
   \vdots           & \vdots            & \ddots & \vdots \\
   0                & 0                 & \cdots & 1      \\
  -(\alpha_0 - f_0) &  -(\alpha_1 - f_1) & \cdots & -(\alpha_{n-1} - f_{n-1})
\end{array}\right]
$$
so we can directly reassign the characteristic polynomial of $A$ by
setting $f_i = \alpha_i - d_i$. More generally,
$$
F_C = B_m^{-1} [ A_{dm} - A_m ]
$$
where $B_m$, $A_{dm}$, $A_m$ are the $m$ $\sigma_j$th rows of $B_C$,
$A_d$, and $A_C$, where $A_d$ is any matrix with the desired
characteristic polynomial. In the multiple-input case such a choice is
not unique for a given set of desired eigenvalues.

In the single-input case we can also apply Ackermann's gain formula:
$$
F = -e_n^\top \mathcal{C}^{-1} \alpha_d(A),
$$
where $\alpha_d(s)$ is the desired characteristic polynomial and
$e_n$ is a matrix with a 1 in row $n$ and zero elsewhere.

\subsubsection{Eigenvector Placement}
It is also possible to assign eigenvectors in a controllable,
multi-input system. Assume that $F$ has been
chosen to place eigenvalues at desired locations.
Choose $M_j$, $D_j$ such that
$$
\left[\begin{array}{r}
  M_j \\ -D_j
\end{array}\right] =
\left[\begin{array}{r}
   P^{-1} S(s) \\
  -B_m^{-1}[\Lambda(s) - A_m S(s)]
\end{array}\right]
$$
forms a basis for the null space of $[s_j I - A, B]$, where $P$ is the
transformation bringing the system to controller form. Here
$\Lambda(s) = \mathrm{diag}([s^{\mu_1}, \dots, s^{\mu_m}])$ and
$$
S(s) =
\mathrm{blk diag}(\{
  [1, s, \dots, s^{\mu_1 - 1}]^\top,
  \dots,
  [1, s, \dots, s^{\mu_m - 1}]^\top
\})
$$
and
$$
A_m =
\left[\begin{array}{c}
  q_1 A^{\mu_1} \\
  \vdots      \\
  q_m A^{\mu_m},
\end{array}\right], \quad
B_m =
\left[\begin{array}{c}
  q_1 B^{\mu_1 - 1} \\
  \vdots      \\
  q_m B^{\mu_m - 1},
\end{array}\right].
$$
Next construct a matrix $V$ with desired eigenvectors as columns,
which is constrained by $V = [M_1 a_1, \dots, M_n a_n]$ for some
vectors $a_i$ and the requirement that conjugate eigenvalues have
conjugate eigenvectors. We also have $W = [D_1 a_1, \dots, D_n a_n]$. Then
finding $F = WV^{-1}$ gives the feedback matrix that places the
eigenvectors as desired.

\subsection{Observer Design}
Suppose we compute a state estimate $\hat{x}$ from the outputs $y$ and
the inputs $u$. Then
$$
\dot{\hat{x}} = A\hat{x} + B u, \quad
e = x - \hat{x}, \quad
\dot{e} = A(x - \hat{x}) = Ae
$$
and so the error of the open loop system will go to 0 only if $A$ has
asymptotically stable eigenvalues. Furthermore the initial state must
be known for this estimator to track the state of the system.

A closed-loop observer takes the form
$$
\dot{\hat{x}} = A\hat{x} + B u + K(y - \hat{y}), \quad
\hat{y} = C \hat{x} + D u, \quad
\dot{e} = \dot{x} - \dot{\hat{x}} = (A - KC)e.
$$
We can then compute the derivative $\dot{\hat{x}}$ by a linear
transformation on $u$ and $y$ and pass it through an integrator to
receive a state estimate $\hat{x}$. This state estimate can then be
used for state feedback control.

Since $\lambda(A - KC) = \lambda(A^\top + C^\top \tilde{F})
\tilde{F}^\top$ where $\tilde{F} = -K^\top$, we can formulate the
closed-loop observer design as the dual of a closed-loop controller
design for the pair $(A^\top, C^\top$, taking the negative transpose
of the feedback matrix we derive. The separation principle guarantees
that controller and observer design are distinct from one another. We
arrive at a controller/observer design with $2n$ states:
$$
\left[\begin{array}{c}
  \dot{x} \\
  \dot{e}
\end{array}\right] =
\left[\begin{array}{r r}
  A + B F &    -B F \\
        0 & A - K C
\end{array}\right]
\left[\begin{array}{c}
  x \\
  e
\end{array}\right]
+
\left[\begin{array}{c}
  B \\
  0
\end{array}\right]v, \quad
y =
\left[\begin{array}{r r}
  C + DF & -DF
\end{array}\right]
\left[\begin{array}{c}
  x \\
  e
\end{array}\right]
+ D v.
$$
We typically want to place the eigenvalues of $A - KC$ about 5 times
farther to the left of 0 so that the error in the observer diminishes
rapidly enough that it does not affect the controller. Placing
eigenvalues too far to the left results in a differentiating observer,
which increases high frequency noise.

In the Laplace domain a closed-loop observer has the form
$$
U(s) = [1 - G_u(s)]^{-1} [G_y Y(s) + V(s)]
$$
so when $v(t) = 0$, the feedback path has the transfer function
$(1 - G_u)^{-1}G_y$.

Linear feedback control can stabilize a system that is both
stabilizable and detectable.

\subsection{Linear Quadratic Regulator}
The performance index is defined as
$$
J = \int_0^{t_f} [x^\top(t) Q x(t) + u^\top (t) R u(t)] \dif t
$$
where $Q$ is symmetric and positive semidefinite, $R$ is symmetric
and positive definite. Write $Q = M^\top M$, where $z = M x$ is the
controlled output. When $u(t) = F x(t)$ and $t_f \to \infty$, we have
$J = x^\top(0) W x(0)$.

Over the interval $[0, t_f]$, $J$ is minimized by
$u(t) = -R^{-1} B^\top P$, where $P$ is the unique
positive semidefinite solution to the
differential Riccati equation
$$
-\frac{\partial P}{\partial t} =
P A + A^\top P + Q - P B R^{-1} B^\top
$$
which in the limit $t_f \to \infty$ reduces to the algebraic Riccati
equation
$$
P A + A^\top P + Q - P B R^{-1} B^\top P = 0
$$
if $(A, B)$ is stabilizable and $(A, M)$ is detectable, in which case
$J$ is finite iff. $\mathfrak{Re}[\lambda(A + BF)] < 0$. The input
$u(t)$ is then optimal among all square integrable functions.

\subsection{Linear Gaussian Estimator (Kalman Filter)}
The dual to the LQR finds the optimal feedback
$\tilde{F} =- K^\top = -R^{-1} \tilde{B}^\top P$ where
$$
\tilde{A} = A^\top, \quad
\tilde{B} = C^\top, \quad
\tilde{F} = -K^\top, \quad
V = R, \quad
\Gamma = M^\top,
P_e = P
$$
and optimizes the observer for the system
$$
\dot{x} = A x + B u + \Gamma w, \quad
y = C x + v
$$
where $w, v$ are independent Gaussian noise processes with zero mean
and variance $W, V$, by solving the Riccati equation
$$
  P_e \tilde{A}^\top
+ \tilde{A} P_e
+ \Gamma \Gamma^\top
- P_e \tilde{C}^\top V^{-1} \tilde{C} P_e
= 0
$$

\subsection{Transfer Functions}
Recall that an uncompensated system has the transfer function
$$
H(s) = C(sI - A)^{-1} B + D.
$$
It can be shown that the closed-loop transfer function with state
feedback is given by
$$
H_F(s) = (C + DF)[sI - (A + BF)]^{-1} B + D = H(s)[I - F(sI - A)^{-1} B]^{-1}.
$$
The transfer function depends only on the parts of the system that are
both controllable and observable:
$$
H(s) = C_1 (sI - A_{11})^{-1} B_1 + D
$$
A system which is both controllable and observable has no pole-zero cancellations.

\subsection{Reduced-Order Observers}
If a subset of $p$ states are available but other $n - p$ must be
estimated from $m$ outputs, it may be possible to design a state
estimator for $n - p$ states. In this case the system can be written
$$
\left[\begin{array}{c}
  \dot{\hat{x}}_1  \\
  \dot{\hat{x}}_2
\end{array}\right]
=
\left[\begin{array}{c c}
  A_{11} & A_{12} \\
  A_{21} & A_{22}
\end{array}\right]
\left[\begin{array}{c}
  x_1 \\
  x_2
\end{array}\right]
+
\left[\begin{array}{c c}
  B_1 \\
  B_2
\end{array}\right] u, \quad
z =
\left[\begin{array}{c}
  x_1 \\
  0
\end{array}\right]
$$
so $\dot{x}_2 = A_{22} x_2 + \tilde{B}\tilde{u}$, where
$\tilde{B} = [A_{21}, B_2]$ and $\tilde{u} = [x_1, u]^\top$. Then
setting
$$
\tilde{y} = \dot{x}_1 - A_{11} x_1 - B_1 u = A_{12} x_2
$$
we construct the estimator
\begin{align*}
\dot{\hat{x}}_2
 &= A_{22} \hat{x}_2
  + \tilde{B} \tilde{u}
  + \tilde{K} (\tilde{y} - A_{12} \hat{x}_2) \\
 &= (A_{22} - \tilde{K} A_{12}) \hat{x}_2
  + (A_{21} + B_2 u)
  + \tilde{K} (\dot{z} - A_{11} z - B_1 u).
\end{align*}
In this case $\dot{e} = (A_{22} - \tilde{K}A_{12})e$ so if $A_{22}$,
$A_{12}$ is observable we can cause this error to diminish by
assigning $\tilde{K}$. This is true if $(A, C)$ is observable.

It is also possible to design such a reduced-order observer when
$y = Cx$ and $\mathrm{rank} C = p$. In this case define a similarity
transformation
$
P =
\left[\begin{array}{c}
  C       \\
  \hat{C}
\end{array}\right]
$
where $\hat{C}$ is chosen to make $P$ nonsingular, and then we have
that $CP^{-1} = [I_p, 0]$. Taking
$$
\bar{x} = P x, \quad
\bar{A} = P A P^{-1}, \quad
\bar{B} = P B, \quad
\bar{C} = C P^{-1}
$$
it is therefore possible to use the same approach as above.

\section{Set-Point Control}

\subsection{Feedforward Set-Point Control}
Consider a system stabilized by state feedback, which has the
following closed loop transfer function and input:
$$
H(s) = C[sI - (A + BF)]^{-1} B, \quad
u = F x + v
$$
With $v = N r$, if $C(A + BF)^{-1}B$ is nonsingular, we can drive the
system asymptotically to the point $r$ by setting
$$
N = -[C(A + BF)^{-1} B]^{-1}.
$$
This is true iff. $(A, B)$ is stabilizable and
$
\mathrm{rank}
\left[\begin{array}{c c}
  A & B \\
  C & 0
\end{array}\right] = n + m.
$
This feedforward set-point control is sensitive to the model
parameters $A, B, C$.

\subsection{Integral Feedback Set-Point Control}
Let
$$
\dot{x} = Ax + Bu + \Gamma w, \quad
y = Cx + Ew
$$
be a system with a constant parameter disturbance $w$. We wish to
drive the system asymptotically to $y = r$. Let $\dot{z} = r - y$ and write
$
\mathcal{X} =
\left[\begin{array}{c}
  x \\ z
\end{array}\right].
$
Then the system
$$
\left[\begin{array}{c}
  \dot{x} \\
  \dot{z}
\end{array}\right]
=
\left[\begin{array}{r r}
  A & 0 \\
 -C & 0
\end{array}\right]
\left[\begin{array}{c}
  x \\
  z
\end{array}\right]
+
\left[\begin{array}{c}
  B \\
  0
\end{array}\right]
u
+
\left[\begin{array}{r}
   \Gamma \\
  -E
\end{array}\right]
w
+
\left[\begin{array}{c}
  0 \\
  I_m
\end{array}\right]
r
$$
is stabilized by feedback control
$$
u = F \mathcal{X} = F_1 x + F_2 z
$$
if and only if $(\mathcal{A}, \mathcal{B})$ above is stabilizable,
which is true iff. $(A, B)$ is stabilizable and
$
\mathrm{rank}
\left[\begin{array}{c c}
  A & B \\
  C & 0
\end{array}\right] = n + m.
$
In this case the dynamics are given by
$$
\mathcal{A} + \mathcal{B} F =
\left[\begin{array}{r r}
  A + BF_1 & BF_2 \\
  -C       & 0
\end{array}\right].
$$
Incorporating an observer we arrive at the closed-loop integral
control system
$$
\dot{z} = r - y, \quad
\dot{\hat{x}} = A \hat{x} + B u + K (y - C \hat{x}), \quad
u = F_1 \hat{x} + F_2 z
$$
with dynamics
$$
\left[\begin{array}{c}
  \dot{x} \\
  \dot{z} \\
  \dot{e}
\end{array}\right] =
\left[\begin{array}{r r r}
  A + B F_1 & B F_2 & -B F_1 \\
  -C        & 0     &  0     \\
   0        & 0     &  A - K C
\end{array}\right]
\left[\begin{array}{c}
  x \\
  z \\
  e
\end{array}\right]
+
\left[\begin{array}{c}
  \quad \\
  \quad
\end{array}\right]
\left[\begin{array}{c}
  w \\
  r
\end{array}\right]
$$
and will asymptotically approach the equilibrium $y = r$ if designed
so that the controller $\mathcal{A} + \mathcal{B} F$ and
observer $A - K C$ have stable eigenvalues.

\section{Realizations}

\end{document}
