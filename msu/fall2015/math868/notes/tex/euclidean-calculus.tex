\section{Euclidean space}

$n$-dimensional Euclidean space
$$
  \mathbb{R}^n
= \{ (x^1, x^2, \dots, x^n) : x^i \in \mathbb{R} \}
$$
is equipped with the \emph{Euclidean norm}
$$
\| x \| = \sqrt{ \sum_{i=1}^n (x^i)^2 }
$$
which induces a metric $d(x, y) = \| x - y \|$
and thus \emph{metric balls}
$$
  B(x, r)
= \{ y \in \mathbb{R}^n : d(x, y) < r \}.
$$
These generate a topology where a set $U \subset \mathbb{R}^n$ is open
if $\forall u \in U$, $\exists \varepsilon > 0$ s.t.
$B(u, \varepsilon) \subset U$.

\begin{defn}[Neighborhood]
A \emph{neighborhood} of a point $x \in \mathbb{R}^n$ is an open set
$U \subset \mathbb{R}^n$ such that $x \in U$.
\end{defn}


\section{Regular functions}

\subsection{Classes of real functions}

\begin{defn}[Real function, differentiability classes]
A \emph{real function} is a function $f : U \to \mathbb{R}$ where
$U$ is open in $\mathbb{R}^n$.

Given $k \in \{ 0 \} \cup \mathbb{N} \cup \{ \infty \}$, $f$ is of
class $C^k$ at $p \in U$ provided that the partial derivatives
$\frac{\partial^j f}{\partial x^{i_1} \cdots \partial x^{i_j}}$ exist
and are continuous at $p$ for all $j \leq k$.
\end{defn}

Note that $f \in C^0$ at $p$ means $f$ is continuous at $p$. $f$ is
said to be $C^k$ on $U$ if this is true for all $p \in U$. Furthermore
$C^{k+1} \implies C^k$, $\forall k$. We also  denote
$$
  C^k(U)
= C^k(U, \mathbb{R})
= \{ f: U \to \mathbb{R} : f \in C^k \}
$$
and say that $f$ is \emph{smooth} on $U$ if $f \in
\mathbb{C}^\infty(U)$.

\begin{xmpl}
$f(x) = x^{1 / 3}$ is $C^1$ on $C^1(\mathbb{R} \setminus \{ 0 \})$.
\end{xmpl}

\begin{theorem}
$$
g(x) = \int_0^x f(t) \dif t
$$
if and only if $g^\prime(x) = f(x)$. Here $g \in C^1(\mathbb{R})$ and
$g \in C^2(\mathbb{R} \setminus \{ 0 \})$, but
$g \notin C^2(\{ 0 \})$. This theorem allows us to construct for any
function in $C^k$ a function that is not in $C^{k+1}$, so these
classes are all distinct.
\end{theorem}

\subsection{Vector valued functions}
We say that a vector-valued function
$f \in C^k(U, \mathbb{R}^m)$ if each component function
$f^i = \pi_i \circ f$ is $C^k(U, \mathbb{R})$.

Given $f \in C^\infty(U, \mathbb{R})$ and $p \in U$,
$f$ has a Taylor series centered at $p = (p^1, \dots, p^n)$ given by
\begin{align*}
  f(x)
&= f(p) \\
&+ \sum_{i=1}^n
    \frac{\partial f}
         {\partial x^i}(p)
    (x^i - p^i) \\
&+ \frac{1}{2!}
    \sum_{i,j}
      \frac{\partial f}
           {\partial x^i \partial x^j}(p)
      (x^i - p^i)
      (x^j - p^j) \\
&+  \cdots \\
&+  \frac{1}{k!}
    \sum_{i_1, \cdots, i_k}
      \frac{\partial^k f}
           {\partial x^{i_1} \cdots \partial x^{i_k}}(p)
      \prod_{i_1, \cdots, i_k} (x^{i_j} - p^{i_j})
\end{align*}

\begin{defn}
A function $f \in C^\infty (U, \mathbb{R})$ is real-analytic at $p \in
U$ if $f(x)$ equals its Taylor series in some neighborhood of $p$.
In this case we write $f \in C^\omega(U, \mathbb{R})$.
\end{defn}

\begin{xmpl}
A function that is smooth but not real-analytic is the bump function
$$
  f(x)
= \left\{
    \begin{array}{l l}
      0             & x \leq 0, \\
      e^{-\frac{1}{x}} & x > 0
    \end{array}
  \right.
$$
This function's Taylor series at 0 is constant 0, but it is not equal
to 0 on any neighborhood of 0.
\end{xmpl}

\begin{defn}[Star-shaped set]
A set $U \subset \mathbb{R}^n$ is said to be \emph{star-shaped} with
respect to a point $p \in U$ if for each point $u \in U$ the line
segment $[u, p] \subset U$. $U$ is convex if it is star-shaped with
respect to every point in $U$.
\end{defn}

In particular note that a metric ball is convex.

\begin{theorem}[Taylor series with remainder]
Let $U \subset \mathbb{R}^n$ be star-shaped with respect to some $p$
and let $f \in C^\infty(U, \mathbb{R})$. Then there exist smooth
functions $g_i \in C^\infty(U, \mathbb{R})$, $i = 1, \dots, n$ such
that
\begin{enumerate}
  \item{
    $$
    f(x) = f(p) + \sum_{i=1}^n g_i(x) (x^i - p^i)
    $$
  }
  \item{
    $$
    g_i(p) = \frac{\partial f}{\partial x^i} (p).
    $$
  }
\end{enumerate}
\end{theorem}

\begin{proof}
Let $x \in U$. Restrict $f$ to the line segment $[p, x]$,
i.e. $\gamma(t) = p + t(x - p)$, $t \in [0, 1]$. Then
$$
  \frac{\dif}{\dif t} (f \circ \gamma) (t)
= \nabla f (p + t(x - p)) \cdot (x - p)
= \sum_{i=1}^n
    \frac{\partial f}
         {\partial x^i}
    (p + t(x - p))
    \cdot
    (x^i - p^i).
$$
Let
$$
  g_i(x)
= \int_0^1
    \frac{\partial f}
         {\partial x^i}
    (p + t(x - p))
    \dif t
$$
and note that since $f \in C^\infty$ so is $g_i$, and
$g_i(p) = \frac{\partial f}{\partial x^i}(p)$. But
\begin{align*}
   f(x) - f(p)
&= \int_0^1
    \frac{\dif}{\dif t}
      f(p + t(x - p)) \\
&= \int_0^1
     \sum_{i=1}^n
       \frac{\partial f}
            {\partial x^i}
       (p + t(x - p))
       \cdot
       (x^i - p^i) \\
&= \sum_{i=1}^n
     g_i(x) (x^i - p^i).
\end{align*}
as desired.
\end{proof}

Take $p = 0$. Then
\begin{align*}
   f(x)
&= f(0) + g_1(x) \cdot x \\
&= f(0) + [g_1(0) + g_2(x) \cdot x) x \\
&= f(0) + g_1(0) x + g_2(x) x^2 \\
&= f(0) + g_1(0) x + [g_2(0) + g_3(x) x] x \\
&= f(0)
 + g_1(0) x
 + g_2(0) x^2
 + \cdots
 + g_i(0) x^i
 + g_{i+1}(0) x^{i+1}
 + \cdots
\end{align*}

\section{Tangent vectors and derivations}

This is a discussion of tangent vectors in Euclidean space, but will
later be generalized to discuss tangent spaces $T_p M$ to a manifold
$M$ at a point $p$.

Let $v \in \mathbb{R}^n$, given by $v = (v^1, \dots, v^n)$. We can
also regard this point as a vector, and we may draw the distinction
with the notation $\langle v^1, \dots, v^n \rangle$. We might instead
consider vectors based at some point $p$ other than the origin.

We write $T_p(\mathbb{R}^n) = T_p \mathbb{R}^n$ to mean the space of
all vectors in $\mathbb{R}^n$ based at a point $p \in
\mathbb{R}^n$. This is a $\mathbb{R}$-vector space, with a vector
space isomorphism $T_p \mathbb{R}^n \simeq \mathbb{R}^n$.

\subsection{Tangent vectors}

\begin{defn}[Directional derivative]
Consider a neighborhood $U$ of $p$, and let $f \in
C^\infty(U, \mathbb{R})$. Consider the one-parameter family of points
$c: \mathbb{R} \to U$ given by
$$
  c(t)
= (p^1 + v^1, \dots, p^n + t v^n)
$$
so that $c(0) = p$, $c^\prime(0) = v$. Then we define the
\emph{directional derivative} of $f$ at $p$ in direction $v$ by
$$
  D_v f
= \left.
    \frac{\dif}{\dif t}
  \right|_{t = 0}
    f(c(t)).
$$
This is a real number measuring the rate of change of $f$ in direction
$v$.
\end{defn}


Recall that
$$
  \mathrm{grad}(f)(p)
= \nabla f(p)
= \left\langle
    \frac{\partial f}
         {\partial x^1}(p),
    \dots,
    \frac{\partial f}
         {\partial x^n}(p)
  \right\rangle
$$
and that the chain rule says that
$$
  D_v f
= \nabla f(p) \cdot v
= \sum_{i=1}^n
    v^i
    \frac{\partial f}
         {\partial x^1}(p).
$$
We can therefore consider
$D_v : C^\infty(U) \to \mathbb{R}$
given by $g \mapsto D_v g$, i.e. we define the operator
$$
  D_v
= \sum_{i=1}^n
    v^i
    \left.
      \frac{\partial}
           {\partial x^i}
    \right|_p.
$$

Note that $C^\infty(U)$ is a $\mathbb{R}$-vector space, and indeed a
$\mathbb{R}$-algebra since we have multiplication of $C^\infty$
functions in a way that respects the vector space
structure. Furthermore, $D_v$ is a linear map. We would therefore like
to make an identification between $v \in T_p \mathbb{R}^n$ with the
operators $D_v$. However, $v$ depends only on the point $p$, while
$C^\infty(U)$ depends on the choice of open set $U$ containing $p$.

\subsection{Germs of smooth functions}

\begin{defn}[Germs of smooth functions]
Consider the set of all pairs
$$
  X_p
= \{ (f, U) : U \text{ is a neighborhood of } p,
              f \in C^\infty (U)
  \}
$$
and define a relation on $X_p$ by
$(f_1, U_1) \sim (f_2, U_2)$ if there exists a neighborhood $U$ of $p$
with $U \subset U_1 \cap U_2$ and $f_1 = f_2$ on $U$. Note that if
$(f_1, U_1) \sim (f_2, U_2)$ then $D_v f_1 = D_v f_2$ for any
$v \in T_p \mathbb{R}^n$, since the derivatives agree on the whole
neighborhood. The \emph{germs of smooth functions at $p$} are the
equivalence classes of $C^\infty(U)$ under this relation, and this
quotient space is denoted $C_p^\infty$.
\end{defn}

It will be shown in homework problems that $C_p^\infty$ is also a
$\mathbb{R}$-vector space, and even a
$\mathbb{R}$-algebra. Furthermore $D_v : C_p^\infty \to \mathbb{R}$ is
linear, though it is not an algebra homomorphism since it operates on
the product of functions by the Leibniz rule
$$
  D_v (f \cdot g)
= D_v (f) \cdot g(p)
+ f(p) \cdot D_v (g),
$$
which is true because
$  D_v
 = \sum_{i=1}^n
     v^i
     \left.
       \frac{\partial}{\partial x^i}
     \right|_p
$ and these partial derivatives satisfy the Leibniz rule.

\begin{defn}[Derivation]
A \emph{point derivation} at $p$ is a linear map
$D : C_p^\infty \to \mathbb{R}$ that satisfies the Leibniz rule
$$
D (f \cdot g) = D(f) \cdot g(p) + f(p) \cdot D(g),
$$
\end{defn}

In the homework it will be shown that the space of point derivations
at $p$, written $\mathcal{D}_p(\mathbb{R}^n)$, is a
$\mathbb{R}$-vector space. There is a function
$\phi: T_p \mathbb{R}^n \to \mathcal{D}_p \mathbb{R}^n$ given by
$$
        v
      = \langle v^1, \dots, v^n \rangle
\mapsto D_v
      = \sum_{i=1}^n
          v^i
          \left. \frac{\partial}{\partial x^i}\right|_p.
$$
In particular the basis vectors
$e_i = \langle 0, 0, \dots, 1, \dots, 0 \rangle$ are mapped to
$D_{e_i} = \left.\frac{\partial}{\partial x^i}\right|_p$. We will
write
$$
  \delta^i_j
= \left\{
    \begin{array}{l c}
      1, & \quad i = j \\
      0, & \quad i \neq j
    \end{array}
  \right.
$$

\begin{theorem}
$\phi : T_p \mathbb{R}^n \to \mathcal{D}_p \mathbb{R}^n \subset
(C_p^\infty \to \mathbb{R})$ given by
$v \mapsto D_v$ or explicitly
$$
  v
= \sum_{i=1}^n v^i e_i
\mapsto
  \sum_{i=1}^n v^i \left.\frac{\partial}{\partial x^i}\right|_p
$$
 is an isomorphism of $\mathbb{R}$-vector spaces.
\end{theorem}

\begin{proof}

\begin{itemize}
  \item{
    $\phi$ is injective.
    Denote the $i$-th coordinate function
    $x^i : \mathbb{R}^n \to \mathbb{R}$, given by $p \to p^i$, and note
    that
    $\left.\frac{\partial}{\partial x^j}\right|_p (x^i) = \delta_i^j$.
    Suppose $\phi(v) = D_v = 0$ for some $v \in T_p \mathbb{R}^n$. Then
    $$
      0
    = D_v (x^j)
    = \sum_{i=1}^n
        v^i
        \left.
          \frac{\partial}
               {\partial x^j}
        \right|_p
          (x^j)
    = v^j,
    $$
    so $\ker \phi = \{ 0 \}$.
  }
  \item{
    $\phi$ is surjective.

    \begin{lemma}
      Let $D \in C_p^\infty$. Then $D(c) = 0$ for all constant
      functions.
    \end{lemma}
    \begin{proof}
      \begin{align*}
           D(c)
        &= D(c \cdot 1)
         = c D(1)
         = c D(1 \cdot 1) \\
        &= c (1 D(1) + 1 D(1))
         = 2 c D(1)
         = 2 D(c)
      \end{align*}
      so $D(c) = 0$.
    \end{proof}

    Let $D \in \mathcal{D}_p \mathbb{R}^n$, $v^i = D(x^i)$, and let
    $[f] \in C_p^\infty$. Choose some representative
    $f : U \to \mathbb{R}$ of this equivalence class, taking $U$ to be
    as small as we like. In particular since the domain is open, we
    may take $U$ to be some small ball which is thus star-shaped, so
    we may use Taylor's theorem with remainder to write
    $$
      f(x)
    = f(p)
    + \sum_{i=1}^n
        g_i(x)
        (x^i - p^i), \quad
      g_i(p)
    = \frac{\partial f}{\partial x^i}(p).
    $$
    Then
    \begin{align*}
       D f
    &= D(f(p))
     + D(\sum g_i (x) (x^i - p^i) \\
    &= \sum
         \left(
           g_i(p) D(x^i - p^i)
         + D(g_i(x))(p^i - p^i)
         \right) \\
    &= \sum
         g_i(p) D(x^i) \\
    &= \sum_{i=1}^n
         D(x^i)
         \frac{\partial f}
              {\partial x^i}(p) \\
    &= \sum_{i=1}^n
         v^i
         \frac{\partial f}
              {\partial x^i}(p) \\
    &= D_v f.
    \end{align*}
  }
\end{itemize}
\end{proof}

\begin{defn}[Vector field]
A \emph{vector field} on a neighborhood $U \subset \mathbb{R}^n$ is a
map $X$ that associates to each $p \in U$ a vector
$X_p \in \mathcal{D}_p \mathbb{R}^n$. We can write a vector field in
coordinates as
$$
X = \sum_{i=1}^n v^i \frac{\partial}{\partial x^i}
$$
where $v^i : U \to \mathbb{R}$. We say that a vector field is smooth
if each of its component functions $v^i$ is smooth.
\end{defn}

Vectors at a point act as point derivations whereas vector fields act
as derivations, i.e. $X$ is a derivation of $C^\infty(U)$,
or $X: C^\infty(U) \to C^\infty(U)$. At each point $p$ we have
$$
(X f)(p) = X_p f,
$$
and since each vector $X_p$ is a point derivation, $X$ as a map
satisfies the Leibniz rule
$$
X (f \cdot g) = f \cdot X(g) + X(f) \cdot g.
$$

\begin{xmpl}
In $\mathbb{R}^2$, with $(x^1, x^2) = (x, y)$ we have a vector field
$$
X = x^2 \frac{\partial}{\partial x} - e^{xy} \frac{\partial}{\\partial y}.
$$
For $f(x, y) = xy$ we have
$$
(Xf)(x, y) = x^2 y - e^{xy} x.
$$
\end{xmpl}

We can compose vector fields $X$, $Y$ by composing maps, but typically
the composition is no longer a derivation. However $XY - YX$ is a
derivation, known as the \emph{Lie bracket} $[X, Y]$ of vector fields.

\section{Tensors and Exterior Algebra of Multicovectors}

Let $V$ be a finite dimensional $\mathbb{R}$ vector space.
For $k \in \mathbb{N}$,
$V^k = \prod_{i=1}^k V$, and
$L_k(V)$ is the set of functions $f: V^k \to \mathbb{R}$ that are
multilinear, i.e. $\mathbb{R}$-linear in each of its $k$ arguments,
i.e. $\forall \alpha \in \mathbb{R}$, $v_i, \bar{v_i} \in V$ we have
$$
  f(v_1, v_2, \dots, \alpha v_i + \bar{v_i}, \dots, v_k)
= \alpha f(v_1, \dots, v_i, \dots, v_k)
+ f(v_1, \dots, \bar{v_i}, \dots, v_k).
$$
This set $L_k(V)$ is a real vector space as well, and elements in this
space are called \emph{(covariant) $k$-tensors}.

For example, $L_1(V) = \mathrm{Hom}(V, \mathbb{R}) = V^\vee$, the dual
space to $V$. Let $\{ e_i \}$ be a basis for $V$. Then each $v \in V$
is a unique sum
$$
  v
= \sum_{i=1}^n v^i e_i.
$$
Define $\alpha^i : V \to \mathbb{R}$ by $v \to v^i$, so that
$$
  v
= \sum_{i=1}^n \alpha^i(v) e_i.
$$
Note that $\alpha^i$ is determined by its values on the basis,
and $\alpha^i(e_j) = \delta_j^i$.

\begin{lemma}
$\{ \alpha^i \}$ is a basis of $L_1(V)$.
\end{lemma}

\begin{proof}
Let $\sum_{i=1}^n c_i \alpha^i = 0$, so that
$0 = \left(\sum_{i=1}^n c_i \alpha^i\right)(e_j) = c_j$,
so each $c_i$ is 0. Hence the $\alpha^i$ are linearly independent.

Let $\omega \in L_1(V)$. Let
$$
  \alpha
= \sum_{i=1}^n \omega(e_i) \alpha^i
\in \mathrm{span}(\alpha^i.
$$
Then $\omega = \alpha$ since $\omega(e_j) = \alpha(e_j)$.
\end{proof}

\begin{xmpl}
For $k = 2$, a the dot product is linear in each of its coordinates.
Let $f : V \times V \to \mathbb{R}$ be the dot product. Then
$$
  f(v, w)
= \sum_{i=1}^n v^i w^i
= \sum_{i=1}^n \alpha^i(v) \alpha^i(w)
$$
where $\{ \alpha^i \}$ is the standard basis for the dual space.
\end{xmpl}

\begin{xmpl}
The determinant $\det : V \times V \times V \to \mathbb{R}$ is linear
in each argument. Explicitly
$$
  \det (v_1, \dots, v_n)
= \sum_{\sigma \in S_n}
    \mathrm{sgn}(\sigma)
    \prod_{i=1}^n
      \alpha^i(v_{\sigma(i)})
$$
where $S_n$ is the symmetric group in $n$ letters.
\end{xmpl}

\begin{defn}[Tensor product]
We have a function
$\otimes : L_k (V) \times L_l(V) \to L_{k + l}(V)$ defined by
$$
  (f \otimes g)
    (v_1, \dots, v_{k+l})
= f(v_1, \dots, v_k) \cdot g(v_{k+1}, \dots, v_l).
$$
We can check that this is linear in each of its components since $f$
and $g$ are and multiplication distributes through addition.
\end{defn}

\subsection{Symmetric groups}
Let $k \in \mathbb{N}$ and denote
$S_k = \mathrm{Perm}(\{1, \dots, k\})$. This is a group with respect
to group composition, with order $k!$. We can represent elements of
$S_k$ by listing the mappings explicitly, by using cycle notation
$$
(1, \sigma(1), \sigma^2(1), \dots)(m, \sigma(m), \sigma^2(m), \dots)
$$
or by a matrix
$$
\left[
  \begin{array}{c c c c}
    1         & 2         & \cdots & k \\
    \sigma(1) & \sigma(2) & \cdots & \sigma(k)
  \end{array}
\right].
$$

Every element in the symmetric group can be written as a product of
transpositions, not uniquely. The number of factors in this
factorization for a given $\sigma \in S_k$ will always have the same
parity. There is a homomorphism
$$
       \mathrm{sgn} : S_k
\to    \{-1, 1\}
\simeq \mathbb{Z} / 2\mathbb{Z}
$$
given by $\mathrm{sgn}(\sigma) = (-1)^{N}$, where $N$ is the number of
transpositions in a factorization of $\sigma$. We can compute
$\mathrm{sgn}(\sigma)$ by writing $\sigma$ as a product of cycles and
counting the number of even-length cycles, then using this as $N$
above.

\begin{lemma}
The symmetric group $S_k$ has an action on tensors $L_k(V)$:
$$
  (\sigma . f)(v_1, \dots, v_k)
= f(v_{\sigma(1)}, \dots, v_{\sigma(k)}).
$$
\end{lemma}
\begin{proof}
This is a group action because $\forall \sigma, \tau \in S_k$ and
$f \in L_k(v)$,
\begin{align*}
   \sigma (\tau f)
&= (\tau f)(v_{\sigma(1)}, \dots, v_{\sigma(k)}) \\
&= (\tau f)(w_1, \dots, w_k) \\
&= f(w_{\tau(1)}, \dots, w_{\tau(k)}) \\
&= f(v_{\sigma(\tau(1))}, \dots, v_{\sigma(\tau(k))})
\end{align*}
letting $w_i = v_{\sigma(i)}$.

This is a linear, invertible map, and indeed a group homomorphism
$S_k \to \mathrm{GL}(L_k(v))$.
\end{proof}

\begin{defn}[Tensor types and operators]
A tensor $f \in L_k(v)$ is said to be \emph{symmetric} if the action
$S_k \to \mathrm{GL}(L_k(v))$ fixes $f$, i.e. $\sigma f = f$ for all
$\sigma \in S_k$, and alternating if
$\sigma f = \mathrm{sgn}(\sigma)f$. We let $S_k(v)$ be the symmetric
tensors and $A_k(v)$ be the tensors that are alternating; we can check
that these are subspaces of $L_k(v)$.

There are symmetrizing and alternating operators
$S, A: L_k(v) \to L_k(v)$ given by
\begin{align*}
   S(f)
&= \sum_{\sigma \in S_k}
     \sigma f, \\
   A(f)
&= \sum_{\sigma \in S_k}
     \mathrm{sgn}(\sigma)
     \sigma f.
\end{align*}
Some authors include a factor of $\frac{1}{k!}$ in these operators.
\end{defn}

\begin{lemma}
  \begin{enumerate}
    \item{
      $S(f) \in S_k(v)$ for all $f$.
    }
    \item{
      If $f \in S_k(v)$, then $S(f) = k! f$.
    }
    \item{
      $A(f) \in A_k(v)$.
    }
    \item{
      If $f \in A_k(v)$, then $A(f) = k! f$.
    }
  \end{enumerate}
\end{lemma}

\begin{proof}
Let $\tau \in S_k$ and $f \in L_k(v)$. Then
\begin{align*}
   \tau A(f)
&= \tau
     \sum_{\sigma \in S_k}
       \mathrm{sgn}(\sigma)
       \sigma f \\
&= \sum_{\sigma \in S_k}
     \mathrm{sgn}(\sigma)
     \tau \sigma f \\
&= \sum_{\sigma \in S_k}
     (\mathrm{sgn}(\tau))^2
     \mathrm{sgn}(\sigma)
     \tau \sigma f \\
&= \mathrm{sgn}(\tau)
   \sum_{\sigma \in S_k}
     \mathrm{sgn}(\tau \sigma)
     \tau \sigma f \\
&= \mathrm{sgn}(\tau) A(f).
\end{align*}

Next let $f \in A_k(v)$. Then
\begin{align*}
   A(f)
&= \sum_{\sigma \in S_k}
     \mathrm{sgn}(\sigma)
     \sigma f \\
&= \sum_{\sigma \in S_k}
     \mathrm{sgn}(\sigma)
     \mathrm{sgn}(\sigma)
     f \\
&= k! f.
\end{align*}
\end{proof}

\subsection{Wedge product}
There is a map
$\wedge : A_k(v) \times A_l(v) \to A_{k + l}(v)$
given by
\begin{align*}
   f \wedge g
&= \frac{1}{k! l!}
     A(f \otimes g).
\end{align*}
Explicitly
\begin{align*}
   (f \wedge g)(v_1, \dots, v_{k+l})
&= \frac{1}{k! l!}
     \sum_{\sigma \in S_{k+l}}
       \mathrm{sgn}(\sigma)
       \sigma(f \otimes g)
         (v_1, \dots, v_{k+l}) \\
&= \frac{1}{k! l!}
     \sum_{\sigma \in S_{k+l}}
       f(v_{\sigma(1)}, \dots, v_{\sigma(k)})
       g(v_{\sigma(k+1)}, \dots, v_{\sigma(k+l)}).
\end{align*}

\begin{xmpl}
  \begin{enumerate}
    \item{
      Let $k = 0$, $l = l$. Then $A_0(v) = \mathbb{R}$, so $f = c$ is a
      constant. Then
      \begin{align*}
         c \wedge g
      &= \frac{1}{l!}
         A(c \otimes g) \\
      &= \frac{c}{l!}
           A(g)
       = c g.
      \end{align*}
    }
    \item{
      Let $k = 1$, $l = 1$. Then
      $A_1(v) = v^\vee$ and
      \begin{align*}
         (f \wedge g)(v_1, v_2)
      &= \frac{1}{1! 1!}
           \left[
             \mathrm{sgn}(\mathrm{id})
             f(v_1)g(v_2)
           + \mathrm{sgn}((1,2))
             f(v_2)g(v_1)
           \right] \\
      &= f(v_1)g(v_2) - f(v_2)g(v_1).
      \end{align*}
    }
    \item{
      Let $k = 2$, $l = 1$. Then
      $$
        S_3
      = \{ \mathrm{id}
         , (1, 2)
         , (1, 2, 3)
         , (1, 3)
         , (1, 3, 2)
         , (2, 3)
        \}.
      $$
      Noe that we have a subgroup $G = \{\mathrm{id}, \tau\}$ with
      left-cosets $G = \{\mathrm{id}, (1,2)\}$,
      $\sigma_1 G = \{ (1, 2, 3), (1, 3) \}$, and
      $\sigma_2 G = \{ (1, 3, 2), (2, 3) \}$. Then
      \begin{align*}
         (f \wedge g)(v_1, v_2, v_3)
      &= \frac{1}{2! 1!}
        (f(v_1, v_2) g(v_3) \\
      &+ \mathrm{sgn}(\tau)
         f(v_2, v_1)
         g(v_3) \\
      &+ \mathrm{sgn}(\sigma_1)
         f(v_2, v_3)
         g(v_1) \\
      &+ \mathrm{sgn}(\sigma_1)
         f(v_3, v_2)
         g(v_1) \\
      &+ \mathrm{sgn}(\sigma_1)
         f(v_3, v_1)
         g(v_1) \\
      &+ \mathrm{sgn}(\sigma_1)
         f(v_1, v_3)
         g(v_1)) \\
      &= \frac{1}{2! 1!}(
         2 f(v_1, v_2) g(v_3)
         2 f(v_2, v_3) g(v_1)
         2 f(v_3, v_1) g(v_2)) \\
      &= f(v_1, v_2) g(v_3)
       - f(v_1, v_3) g(v_2)
       + f(v_2, v_3) g(v_1).
      \end{align*}

      Following this example, in general we have a subgroup of
      $S_{k+l}$ that is isomorphic to $S_k \times S_l$, i.e.
      any $\tau$ in this subgroup can be written as
      $\psi(\tau) = (\tau_1, \tau_2)$, where $\tau_1 \in S_k$ and
      $\tau_2 \in S_l$. Then
      \begin{align*}
         \mathrm{sgn}(\sigma \tau)
         \sigma \tau (f \otimes g)
         (v_1, \dots, v_{k +l})
      &= \mathrm{sgn}(\sigma \tau)
         f(v_{\sigma(\tau_1(1))}, \dots, v_{\sigma(\tau_1(k))})
         g(v_{\sigma(\tau_2(k+1))}, \dots, v_{\sigma(\tau_2(k+l))}) \\
      &= \mathrm{sgn}(\sigma \tau)
         \tau_1
           f(v_{\sigma(1)}, \dots, v_{\sigma(k)})
           g(v_{\sigma(k+1)}, \dots, v_{\sigma(k+l)}) \\
      &= \mathrm{sgn}(\sigma)
          \sigma (f \otimes g)
      \end{align*}
    }
  \end{enumerate}

  For any $\sigma \in S_{k + l}$ and $(v_1, \dots, v_{k+l})$, the
  $k!l!$ summands of the form
  $$
  \mathrm{sgn}(\sigma \tau)
  (\sigma \tau)(f \otimes g)(v_1, \dots, v_{k+l})
  $$
  with $\tau \in G < S_{k + l}$, where
  $G \simeq S_k \times S_l$ consists of the permutations that fix
  the sets $\{1, 2, \dots k\}$ and $\{ k + 1, \dots, k + l \}$ (but
  possibly permutes these sets).
\end{xmpl}

\begin{defn}
A permutation $\sigma \in S_{k+l}$ is a \emph{$(k, l)$-shuffle} if
$$
\sigma(1) < \cdots < \sigma(k)
$$
and
$$
\sigma(k) < \cdots < \sigma(k + l).
$$
\end{defn}
Note that there is precisely one $(k, l)$-shuffle in each of the left
cosets of $G$. This is clear for the coset
$\mathrm{id} \cdot G$ since there is of course only one way to order
$\{ 1, \dots, k \}$ and $\{ k, \dots, k + l \}$. Similarly elements in
$\sigma G$ permute elements in the sets $\sigma(\{ 1, \dots, k \})$
and $\sigma( \{ k + 1, \dots, k + l \})$.

\begin{prop}
For $f \in A_k(v)$, $g \in A_l(v)$, we have
$$
  f \wedge g
= \sum_{\sigma \text{a $(k, l)$ shuffle}}
    \mathrm{sgn}(\sigma)
    \sigma(f \otimes g).
$$
Note also that there are
$$
  { k + l \choose k }
= { k + l \choose l }
+ \frac{(k + l)!}{k! l!}
$$
$(k, l)$ shuffles total, i.e. the order $[S_{k + l} : G]$.
\end{prop}

\subsection{Properties of $\wedge$}
\begin{enumerate}
  \item{
    $\mathbb{R}$-bilinearity, i.e.
    $$
      (\alpha_1 f_1 + \alpha_2 f_2)  \wedge g
    = \alpha_1 (f_1 \wedge g) + \alpha_2 (f_2 \wedge g)
    $$
    and
    $$
      g \wedge (\alpha_1 f_1 + \alpha_2 f_2)
    = \alpha_1 (g \wedge f_1) + \alpha_2 (g \wedge f_2).
    $$
  }
  \item{
    Graded commutativity, i.e.
    $f \wedge g = (-1)^{kl} g \wedge f$.
  }
  \item{
    Associativity, i.e.
    $(f \wedge g) \wedge h = f \wedge (g \wedge h)$.
  }
\end{enumerate}

To prove graded commutativity, note that swapping the first $l$
numbers with the last $k + l$ is a permutation $\tau$ with sign
$(-1)^{kl}$. Then
\begin{align*}
   \frac{1}{k! l!}
   \sum_{\sigma \in S_{k + l}}
     \mathrm{sgn}(\sigma)
     f(v_{\sigma \tau(l + 1)}, \dots, v_{\sigma \tau(l + k)})
     g(v_{\sigma \tau (1)}, \dots, v_{\sigma \tau_{l}}) \\
&= \frac{1}{k! l!}
   \sum_{\sigma}
     \mathrm{sgn}(\sigma)
     (\sigma \tau)
       (g \otimes f)
         (v_{1}, \dots, v_{k + l}) \\
&= \frac{\mathrm{sgn}(\tau)}
        {k! l!}
   \sum_\sigma
     \mathrm{sgn}(\sigma \tau)
     (\sigma \tau)
       (g \otimes f)
         (v_1, \dots, v_{k+l}) \\
&= (-1)^{kl} (g \wedge f) (v_1, \dots, v_{k+l}).
\end{align*}

\begin{lemma}
For $f \in L_k(v), g \in L_l(v)$,
\begin{enumerate}
  \item{
    $A(A(f) \otimes g) = k! A(f \otimes g)$,
  }
  \item{
    $A(f \otimes A(g)) = l! A(f \otimes g)$.
  }
\end{enumerate}
\end{lemma}

\begin{proof}
Note that
$$
       S_k
\simeq S_k^\ast
=      \{ \sigma \in S_{k+l}
        | \sigma(k+i) = k + i, i \in \{1, \dots, l\}
       \}
$$
so
\begin{align*}
   A(A(f) \otimes g)
&= A\left(
     \sum_{\tau \in S_k}
       \mathrm{sgn}(\tau)
       (\tau f) \otimes g
   \right) \\
&= A\left(
     \sum_{\tau \in S_k^\ast}
       \mathrm{sgn}(\tau)
       \tau (f \otimes g)
   \right) \\
&= \sum_{\sigma \in S_{k+l}}
     \sum_{\tau \in S_k^\ast}
       \mathrm{sgn}(\sigma \tau)
       (\sigma \tau)(f \otimes g) \\
&= k!
   \sum_{\mu \in S_{k+l}}
     \mathrm{sgn}(\mu)
     \mu(f \otimes g)
\end{align*}
since for all $\tau \in S_k^\ast$,
$$
  S_{k+l} \tau
= \{ \sigma \tau | \sigma \in S_{k+l} \}
= S_{k + l}
$$
and so for any $\mu \in S_{k + l}$, $\mu$ appears $k!$ times in the
double sum above, once as $\sigma \tau$ for each $\tau \in S^\ast$.
\end{proof}

Assuming this, we have
\begin{align*}
   (f \wedge g) \wedge h
&= \frac{1}{(k + l)! m!}
   A((f \wedge g) \otimes h) \\
&= \frac{1}{(k + l)! m!}
   A\left(
     \frac{1}{k! l!}
     A(f \otimes g) \otimes h
   \right) \\
&= \frac{1}{(k + l)! k! l! m!}
   A(A(f \otimes g) \otimes h) \\
&= \frac{1}{k! l! m!}
   A((f \otimes g) \otimes h),
\end{align*}
and we can show similarly that
$$
  f \wedge (g \wedge h)
= \frac{1}{k! l! m!}
  A(f \otimes (g \otimes h)),
$$
and use the fact that $\otimes$ is associative.

\begin{corol}
Let $f_1, \dots, f_l$ be alternating tensors of degrees
$k_1, \dots, k_l$. Then
$$
  \bigwedge_{i=1}^l f_i
= \frac{1}{\prod_{i=1}^l (k_i)!}
  A\left(\bigotimes_{i=1}^l f_i\right).
$$
\end{corol}

\begin{defn}[Grassmann/Exterior Algebra]
The \emph{Grassmann algebra} is the vector space
$$
  A(V)
= \bigoplus_{k=0}^\infty
    A_k(V),
$$
and we will see that $A_k(V) = \{ 0 \}$ when $k > n$.
This is a real vector space where elements are finite sums of
$v_i \in A_i(V)$, and multiplication in this algebra is given by the
graded commutative product $\wedge$.
\end{defn}

\begin{lemma}
Let $\beta^1, \dots \beta^k \in L_1(V) = A_1(V)$ and
$(v_1, \dots, v_k) \in V^k$. Then
$$
B = [b_j^i] = \beta^i(v_j)
$$
satisfies
$$
  (\beta^1 \wedge \cdots \wedge \beta^k)(v_1, \dots, v_k)
= \det B.
$$
\end{lemma}
\begin{proof}
We have
\begin{align*}
   (\beta^1 \wedge \cdots \wedge \beta^k)(v_1, \dots, v_k)
&= \frac{1}{1! \cdots 1!}
   A(\beta^1 \otimes \cdots \beta^k)(v_1, \dots, v_k) \\
&= \sum_{\sigma \in S_k}
     \mathrm{sgn}(\sigma)
     \sigma(\beta^1 \otimes \cdots \otimes \beta^k)
       (v_1, \dots, v_k) \\
&= \sum_{\sigma \in S_k}
     \mathrm{sgn}(\sigma)
     \beta^1(v_{\sigma(1)}) \cdots \beta^1(v_{\sigma(k)}) \\
&= \sum_{\sigma \in S_k}
     \mathrm{sgn}(\sigma)
     b^1_{\sigma(1)} \cdots b^k_{\sigma(k)} \\
&\triangleq \det B.
\end{align*}
\end{proof}
