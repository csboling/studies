\section{Tensors and Exterior Algebra of Multicovectors}

Let $V$ be a finite dimensional $\mathbb{R}$ vector space.
For $k \in \mathbb{N}$,
$V^k = \prod_{i=1}^k V$, and
$L_k(V)$ is the set of functions $f: V^k \to \mathbb{R}$ that are
multilinear, i.e. $\mathbb{R}$-linear in each of its $k$ arguments,
i.e. $\forall \alpha \in \mathbb{R}$, $v_i, \bar{v_i} \in V$ we have
$$
  f(v_1, v_2, \dots, \alpha v_i + \bar{v_i}, \dots, v_k)
= \alpha f(v_1, \dots, v_i, \dots, v_k)
+ f(v_1, \dots, \bar{v_i}, \dots, v_k).
$$
This set $L_k(V)$ is a real vector space as well, and elements in this
space are called \emph{(covariant) $k$-tensors}. Denote the invertible
such maps by $\mathrm{GL}(L_k(V))$.

For example, $L_1(V) = \mathrm{Hom}(V, \mathbb{R}) = V^\vee$, the dual
space to $V$. Let $\{ e_i \}$ be a basis for $V$. Then each $v \in V$
is a unique sum
$$
  v
= \sum_{i=1}^n v^i e_i.
$$
Define $\alpha^i : V \to \mathbb{R}$ by $v \to v^i$, so that
$$
  v
= \sum_{i=1}^n \alpha^i(v) e_i.
$$
Note that $\alpha^i$ is determined by its values on the basis,
and $\alpha^i(e_j) = \delta_j^i$.

\begin{lemma}
$\{ \alpha^i \}$ is a basis of $L_1(V)$.
\end{lemma}

\begin{proof}
Let $\sum_{i=1}^n c_i \alpha^i = 0$, so that
$0 = \left(\sum_{i=1}^n c_i \alpha^i\right)(e_j) = c_j$,
so each $c_i$ is 0. Hence the $\alpha^i$ are linearly independent.

Let $\omega \in L_1(V)$. Let
$$
  \alpha
= \sum_{i=1}^n \omega(e_i) \alpha^i
\in \mathrm{span}(\alpha^i).
$$
Then $\omega = \alpha$ since $\omega(e_j) = \alpha(e_j)$.
\end{proof}

\begin{xmpl}
For $k = 2$, a the dot product is linear in each of its coordinates.
Let $f : V \times V \to \mathbb{R}$ be the dot product. Then
$$
  f(v, w)
= \sum_{i=1}^n v^i w^i
= \sum_{i=1}^n \alpha^i(v) \alpha^i(w)
$$
where $\{ \alpha^i \}$ is the standard basis for the dual space.
\end{xmpl}

\begin{xmpl}
The determinant $\det : \overbrace{V \times \cdots \times V}^n \to \mathbb{R}$ is linear
in each argument. Explicitly
$$
  \det (v_1, \dots, v_n)
= \sum_{\sigma \in S_n}
    \mathrm{sgn}(\sigma)
    \prod_{i=1}^n
      \alpha^i(v_{\sigma(i)})
$$
where $S_n$ is the symmetric group in $n$ letters.
\end{xmpl}

\begin{defn}[Tensor product]
We have a function
$\otimes : L_k (V) \times L_l(V) \to L_{k + l}(V)$ defined by
$$
  (f \otimes g)
    (v_1, \dots, v_{k+l})
= f(v_1, \dots, v_k) \cdot g(v_{k+1}, \dots, v_l).
$$
We can check that this is linear in each of its components since $f$
and $g$ are and multiplication distributes through addition.
\end{defn}

\subsection{Symmetric groups}
Let $k \in \mathbb{N}$ and denote
$S_k = \mathrm{Perm}(\{1, \dots, k\})$. This is a group with respect
to function composition, with order $k!$. We can represent elements of
$S_k$ by listing the mappings explicitly, by using cycle notation
$$
(1, \sigma(1), \sigma^2(1), \dots)(m, \sigma(m), \sigma^2(m), \dots)
$$
or by a matrix
$$
\left[
  \begin{array}{c c c c}
    1         & 2         & \cdots & k \\
    \sigma(1) & \sigma(2) & \cdots & \sigma(k)
  \end{array}
\right].
$$

Every element in the symmetric group can be written as a product of
transpositions, not necessarily uniquely. The number of factors in this
factorization for a given $\sigma \in S_k$ will always have the same
parity. There is a homomorphism
$$
       \mathrm{sgn} : S_k
\to    \{-1, 1\}
\simeq \mathbb{Z} / 2\mathbb{Z}
$$
given by $\mathrm{sgn}(\sigma) = (-1)^{N}$, where $N$ is the number of
transpositions in a factorization of $\sigma$. We can compute
$\mathrm{sgn}(\sigma)$ by writing $\sigma$ as a product of cycles and
counting the number of even-length cycles, then using this as $N$
above.

\begin{lemma}
The symmetric group $S_k$ has a group action on tensors $L_k(V)$:
$$
  (\sigma . f)(v_1, \dots, v_k)
= f(v_{\sigma(1)}, \dots, v_{\sigma(k)}).
$$
\end{lemma}
\begin{proof}
This is a group action because $\forall \sigma, \tau \in S_k$ and
$f \in L_k(V)$,
\begin{align*}
   \sigma . (\tau . f)
&= (\tau . f)(v_{\sigma(1)}, \dots, v_{\sigma(k)}) \\
&= (\tau . f)(w_1, \dots, w_k) \\
&= f(w_{\tau(1)}, \dots, w_{\tau(k)}) \\
&= f(v_{\sigma(\tau(1))}, \dots, v_{\sigma(\tau(k))}) \\
&= (\sigma \tau . f)(v_1, \dots, v_k)
\end{align*}
letting $w_i = v_{\sigma(i)}$.

This is a linear, invertible map, and indeed a group homomorphism
$S_k \to \mathrm{GL}(L_k(V))$.
\end{proof}

\begin{defn}[Tensor types and operators]
A tensor $f \in L_k(V)$ is said to be \emph{symmetric} if the action
$S_k \to \mathrm{GL}(L_k(V))$ fixes $f$, i.e. $\sigma f = f$ for all
$\sigma \in S_k$, and alternating if
$\sigma f = \mathrm{sgn}(\sigma)f$. We let $S_k(V)$ be the symmetric
tensors and $A_k(V)$ be the tensors that are alternating; we can check
that these are subspaces of $L_k(V)$.

There are symmetrizing and alternating operators
$S, A: L_k(V) \to L_k(V)$ given by
\begin{align*}
   S(f)
&= \sum_{\sigma \in S_k}
     \sigma f, \\
   A(f)
&= \sum_{\sigma \in S_k}
     \mathrm{sgn}(\sigma)
     \sigma f.
\end{align*}
Some authors include a factor of $\frac{1}{k!}$ in these operators.
\end{defn}

\begin{lemma}
The symmetrizing and alternating operators have the following properties:
  \begin{enumerate}
    \item{
      $S(f) \in S_k(V)$ for all $f$.
    }
    \item{
      If $f \in S_k(V)$, then $S(f) = k! f$.
    }
    \item{
      $A(f) \in A_k(V)$.
    }
    \item{
      If $f \in A_k(V)$, then $A(f) = k! f$.
    }
  \end{enumerate}
\end{lemma}

\begin{proof}
Let $\tau \in S_k$ and $f \in L_k(V)$. Then
\begin{align*}
   \tau A(f)
&= \tau
     \sum_{\sigma \in S_k}
       \mathrm{sgn}(\sigma)
       \sigma f \\
&= \sum_{\sigma \in S_k}
     \mathrm{sgn}(\sigma)
     \tau \sigma f \\
&= \sum_{\sigma \in S_k}
     (\mathrm{sgn}(\tau))^2
     \mathrm{sgn}(\sigma)
     \tau \sigma f \\
&= \mathrm{sgn}(\tau)
   \sum_{\sigma \in S_k}
     \mathrm{sgn}(\tau \sigma)
     \tau \sigma f \\
&= \mathrm{sgn}(\tau) A(f).
\end{align*}

Next let $f \in A_k(V)$. Then
\begin{align*}
   A(f)
&= \sum_{\sigma \in S_k}
     \mathrm{sgn}(\sigma)
     \sigma f \\
&= \sum_{\sigma \in S_k}
     \mathrm{sgn}(\sigma)
     \mathrm{sgn}(\sigma)
     f \\
&= k! f.
\end{align*}
\end{proof}

\subsection{Wedge product}
There is a map
$\wedge : A_k(V) \times A_l(V) \to A_{k + l}(V)$
given by
\begin{align*}
   f \wedge g
&= \frac{1}{k! l!}
     A(f \otimes g).
\end{align*}
Explicitly
\begin{align*}
   (f \wedge g)(v_1, \dots, v_{k+l})
&= \frac{1}{k! l!}
     \sum_{\sigma \in S_{k+l}}
       \mathrm{sgn}(\sigma)
       \sigma(f \otimes g)
         (v_1, \dots, v_{k+l}) \\
&= \frac{1}{k! l!}
     \sum_{\sigma \in S_{k+l}}
       f(v_{\sigma(1)}, \dots, v_{\sigma(k)})
       g(v_{\sigma(k+1)}, \dots, v_{\sigma(k+l)}).
\end{align*}

\begin{xmpl}
  \begin{enumerate}
    \item{
      Let $k = 0$, $l = l$. Then $A_0(V) = \mathbb{R}$, so $f = c$ is a
      constant. Then
      \begin{align*}
         c \wedge g
      &= \frac{1}{l!}
         A(c \otimes g) \\
      &= \frac{c}{l!}
           A(g)
       = c g.
      \end{align*}
    }
    \item{
      Let $k = 1$, $l = 1$. Then
      $A_1(V) = V^\vee$ and
      \begin{align*}
         (f \wedge g)(v_1, v_2)
      &= \frac{1}{1! 1!}
           \left[
             \mathrm{sgn}(\mathrm{id})
             f(v_1)g(v_2)
           + \mathrm{sgn}((1,2))
             f(v_2)g(v_1)
           \right] \\
      &= f(v_1)g(v_2) - f(v_2)g(v_1).
      \end{align*}
    }
    \item{
      Let $k = 2$, $l = 1$. Then
      $$
        S_3
      = \{ \mathrm{id}
         , (1, 2)
         , (1, 2, 3)
         , (1, 3)
         , (1, 3, 2)
         , (2, 3)
        \}.
      $$
      Note that we have a subgroup $G = \{\mathrm{id}, \tau\}$ with
      left-cosets $G = \{\mathrm{id}, (1,2)\}$,
      $\sigma_1 G = \{ (1, 2, 3), (1, 3) \}$, and
      $\sigma_2 G = \{ (1, 3, 2), (2, 3) \}$. Then
      \begin{align*}
         (f \wedge g)(v_1, v_2, v_3)
      &= \frac{1}{2! 1!} \\
      (& f(v_1, v_2) g(v_3) \\
      &+ \mathrm{sgn}(\tau)
         f(v_2, v_1)
         g(v_3) \\
      &+ \mathrm{sgn}(\sigma_1)
         f(v_2, v_3)
         g(v_1) \\
      &+ \mathrm{sgn}(\sigma_1)
         f(v_3, v_2)
         g(v_1) \\
      &+ \mathrm{sgn}(\sigma_2)
         f(v_3, v_1)
         g(v_1) \\
      &+ \mathrm{sgn}(\sigma_2)
         f(v_1, v_3)
         g(v_1)) \\
      &= \frac{1}{2! 1!}(
         2 f(v_1, v_2) g(v_3)
       + 2 f(v_2, v_3) g(v_1)
       + 2 f(v_3, v_1) g(v_2)) \\
      &= f(v_1, v_2) g(v_3)
       - f(v_1, v_3) g(v_2)
       + f(v_2, v_3) g(v_1).
      \end{align*}

      Following this example, in general we have a subgroup of
      $S_{k+l}$ that is isomorphic to $S_k \times S_l$, i.e.
      any $\tau$ in this subgroup can be written as
      $\psi(\tau) = (\tau_1, \tau_2)$, where $\tau_1 \in S_k$ and
      $\tau_2 \in S_l$. Then
      \begin{align*}
         \mathrm{sgn}(\sigma \tau)
         \sigma \tau (f \otimes g)
         (v_1, \dots, v_{k +l})
      &= \mathrm{sgn}(\sigma \tau)
         f(v_{\sigma(\tau_1(1))}, \dots, v_{\sigma(\tau_1(k))})
         g(v_{\sigma(\tau_2(k+1))}, \dots, v_{\sigma(\tau_2(k+l))}) \\
      &= \mathrm{sgn}(\sigma \tau)
         \tau_1
           f(v_{\sigma(1)}, \dots, v_{\sigma(k)})
           g(v_{\sigma(k+1)}, \dots, v_{\sigma(k+l)}) \\
      &= \mathrm{sgn}(\sigma)
          \sigma (f \otimes g)
      \end{align*}
    }
  \end{enumerate}

  For any $\sigma \in S_{k + l}$ and $(v_1, \dots, v_{k+l})$, the
  $k!l!$ summands of the form
  $$
  \mathrm{sgn}(\sigma \tau)
  (\sigma \tau)(f \otimes g)(v_1, \dots, v_{k+l})
  $$
  with $\tau \in G < S_{k + l}$, where
  $G \simeq S_k \times S_l$ consists of the permutations that fix
  the sets $\{1, 2, \dots k\}$ and $\{ k + 1, \dots, k + l \}$ (but
  possibly permutes these sets).
\end{xmpl}

\begin{defn}
A permutation $\sigma \in S_{k+l}$ is a \emph{$(k, l)$-shuffle} if
$$
\sigma(1) < \cdots < \sigma(k)
$$
and
$$
\sigma(k) < \cdots < \sigma(k + l).
$$
\end{defn}
Note that there is precisely one $(k, l)$-shuffle in each of the left
cosets of $G$. This is clear for the coset
$\mathrm{id} \cdot G$ since there is of course only one way to order
$\{ 1, \dots, k \}$ and $\{ k, \dots, k + l \}$. Similarly elements in
$\sigma G$ permute elements in the sets $\sigma(\{ 1, \dots, k \})$
and $\sigma( \{ k + 1, \dots, k + l \})$.

\begin{prop}
For $f \in A_k(V)$, $g \in A_l(V)$, we have
$$
  f \wedge g
= \sum_{\sigma \text{a $(k, l)$ shuffle}}
    \mathrm{sgn}(\sigma)
    \sigma(f \otimes g).
$$
Note also that there are
$$
  { k + l \choose k }
= { k + l \choose l }
+ \frac{(k + l)!}{k! l!}
$$
$(k, l)$ shuffles total, i.e. the order $[S_{k + l} : G]$.
\end{prop}

\subsection{Properties of the wedge product}
\begin{enumerate}
  \item{
    $\mathbb{R}$-bilinearity, i.e.
    $$
      (\alpha_1 f_1 + \alpha_2 f_2)  \wedge g
    = \alpha_1 (f_1 \wedge g) + \alpha_2 (f_2 \wedge g)
    $$
    and
    $$
      g \wedge (\alpha_1 f_1 + \alpha_2 f_2)
    = \alpha_1 (g \wedge f_1) + \alpha_2 (g \wedge f_2).
    $$
  }
  \item{
    Graded commutativity, i.e.
    $f \wedge g = (-1)^{kl} g \wedge f$.
  }
  \item{
    Associativity, i.e.
    $(f \wedge g) \wedge h = f \wedge (g \wedge h)$.
  }
\end{enumerate}

To prove graded commutativity, note that swapping the first $l$
numbers with the last $k + l$ is a permutation $\tau$ with sign
$(-1)^{kl}$. Then
\begin{align*}
   (f \wedge g)(v_1, \dots, v_k)
&= \frac{1}{k! l!}
   \sum_{\sigma \in S_{k + l}}
     \mathrm{sgn}(\sigma)
     f(v_{\sigma \tau(l + 1)}, \dots, v_{\sigma \tau(l + k)})
     g(v_{\sigma \tau (1)}, \dots, v_{\sigma \tau_{l}}) \\
&= \frac{1}{k! l!}
   \sum_{\sigma}
     \mathrm{sgn}(\sigma)
     (\sigma \tau)
       (g \otimes f)
         (v_{1}, \dots, v_{k + l}) \\
&= \frac{\mathrm{sgn}(\tau)}
        {k! l!}
   \sum_\sigma
     \mathrm{sgn}(\sigma \tau)
     (\sigma \tau)
       (g \otimes f)
         (v_1, \dots, v_{k+l}) \\
&= (-1)^{kl} (g \wedge f) (v_1, \dots, v_{k+l}).
\end{align*}

\begin{lemma}
For $f \in L_k(V), g \in L_l(V)$,
\begin{enumerate}
  \item{
    $A(A(f) \otimes g) = k! A(f \otimes g)$,
  }
  \item{
    $A(f \otimes A(g)) = l! A(f \otimes g)$.
  }
\end{enumerate}
\end{lemma}

\begin{proof}
Note that
$$
       S_k
\simeq S_k^\ast
=      \{ \sigma \in S_{k+l}
        | \sigma(k+i) = k + i, i \in \{1, \dots, l\}
       \}
$$
so
\begin{align*}
   A(A(f) \otimes g)
&= A\left(
     \sum_{\tau \in S_k}
       \mathrm{sgn}(\tau)
       (\tau f) \otimes g
   \right) \\
&= A\left(
     \sum_{\tau \in S_k^\ast}
       \mathrm{sgn}(\tau)
       \tau (f \otimes g)
   \right) \\
&= \sum_{\sigma \in S_{k+l}}
     \sum_{\tau \in S_k^\ast}
       \mathrm{sgn}(\sigma \tau)
       (\sigma \tau)(f \otimes g) \\
&= k!
   \sum_{\mu \in S_{k+l}}
     \mathrm{sgn}(\mu)
     \mu(f \otimes g)
\end{align*}
since for all $\tau \in S_k^\ast$,
$$
  S_{k+l} \tau
= \{ \sigma \tau | \sigma \in S_{k+l} \}
= S_{k + l}
$$
and so for any $\mu \in S_{k + l}$, $\mu$ appears $k!$ times in the
double sum above, once as $\sigma \tau$ for each $\tau \in S^\ast$.
\end{proof}

Assuming this, we have
\begin{align*}
   (f \wedge g) \wedge h
&= \frac{1}{(k + l)! m!}
   A((f \wedge g) \otimes h) \\
&= \frac{1}{(k + l)! m!}
   A\left(
     \frac{1}{k! l!}
     A(f \otimes g) \otimes h
   \right) \\
&= \frac{1}{(k + l)! k! l! m!}
   A(A(f \otimes g) \otimes h) \\
&= \frac{1}{k! l! m!}
   A((f \otimes g) \otimes h),
\end{align*}
and we can show similarly that
$$
  f \wedge (g \wedge h)
= \frac{1}{k! l! m!}
  A(f \otimes (g \otimes h)),
$$
and use the fact that $\otimes$ is associative.

\begin{corol}
Let $f_1, \dots, f_l$ be alternating tensors of degrees
$k_1, \dots, k_l$. Then
$$
  \bigwedge_{i=1}^l f_i
= \frac{1}{\prod_{i=1}^l (k_i)!}
  A\left(\bigotimes_{i=1}^l f_i\right).
$$
\end{corol}

\begin{defn}[Grassmann/Exterior Algebra]
The \emph{Grassmann algebra} is the vector space
$$
  A(V)
= \bigoplus_{k=0}^\infty
    A_k(V),
$$
and we will see that $A_k(V) = \{ 0 \}$ when $k > n$.
This is a real vector space where elements are finite sums of
$v_i \in A_i(V)$, and multiplication in this algebra is given by the
graded commutative product $\wedge$.
\end{defn}

\begin{lemma}
Let $\beta^1, \dots \beta^k \in L_1(V) = A_1(V)$ and
$(v_1, \dots, v_k) \in V^k$. Then
$$
B = [b_j^i] = [\beta^i(v_j)]
$$
satisfies
$$
  (\beta^1 \wedge \cdots \wedge \beta^k)(v_1, \dots, v_k)
= \det B.
$$
\end{lemma}
\begin{proof}
We have
\begin{align*}
   (\beta^1 \wedge \cdots \wedge \beta^k)(v_1, \dots, v_k)
&= \frac{1}{1! \cdots 1!}
   A(\beta^1 \otimes \cdots \beta^k)(v_1, \dots, v_k) \\
&= \sum_{\sigma \in S_k}
     \mathrm{sgn}(\sigma)
     \sigma(\beta^1 \otimes \cdots \otimes \beta^k)
       (v_1, \dots, v_k) \\
&= \sum_{\sigma \in S_k}
     \mathrm{sgn}(\sigma)
     \beta^1(v_{\sigma(1)}) \cdots \beta^1(v_{\sigma(k)}) \\
&= \sum_{\sigma \in S_k}
     \mathrm{sgn}(\sigma)
     b^1_{\sigma(1)} \cdots b^k_{\sigma(k)} \\
&\triangleq \det B.
\end{align*}
\end{proof}

\subsection*{Multi-index notation}
We write $I = (i_1, \dots, i_k)$ and $i_m \in \{1, \dots, n\}$ to
denote an increasing set
$$
1 \leq i_1 < \cdots < i_k \leq n,
$$
e.g.
\begin{align*}
   e_I
&= (e_{i_1}, \dots, e_{i_k}) \\
   \alpha^I
&= \alpha^{i_1} \wedge \cdots \wedge \alpha^{i_k} \\
   \frac{\partial^k}{\partial x^I}(f)
&= \frac{\partial^k f}
        {\partial x^{i_1} \cdots \partial x^{i_k}} \\
   \delta^{I}_J
&= \left\{
     \begin{array}{c c}
       1, & \quad i_1 = j_1, \dots, i_k = j_k \\
       0, & \quad \text{otherwise}
     \end{array}
   \right\}
\end{align*}

\subsection*{A basis for $A_k(V)$}

\begin{lemma}
Let $|I| = |J| = k$. Then $\alpha^I(e_J) = \delta^I_J$.
\end{lemma}

\begin{proof}
We have
$$
\alpha^I(e_J) = \det(b_n^m)
$$
where $b_n^m = \alpha^{i_m}(e_{j_n})$.

If $I = J$ then $b_n^m = \delta_n^m$ and so
$\det (b_n^m) = \det(\mathrm{Id}_k) = 1$.

If $I \neq J$ then $[b_n^m]$ has a row of all zeroes due to the
following argument. Without loss of
generality, there is some $m$ such that
$$
i_1 = j_1, \dots, i_{m-1} = j_{m-1}, i_m < j_m.
$$
Furthermore $i_m$ is distinct from $j_1, \dots, j_k$, since
$$
j_1 < \cdots < j_{m-1} = i_{m-1} < i_m < j_m < j_{m+1} < \cdots
$$
so $\alpha^{i_m}(e_{j_l}) = 0$ for any $l$. Therefore the $m$ row of
the matrix $B$ is zero, hence $\det B = 0$.
\end{proof}

\begin{remark}
If $\omega \in A_k(V)$, then $\omega$ is determined on its values
$\omega(e_I)$ with $|I| = k$ and $I$ increasing, i.e.
$\omega_1(e_I) = \omega_2(e_I) \implies \omega_1 = \omega_2$,
because $\omega$ is multilinear and alternating.

\begin{xmpl}
Let $\omega \in A_2(\mathbb{R}^3)$. Then
$$
  \omega(v, w)
= \omega(v^1 e_1 + v^2 e_2 + v^3 e_3,
         w^1 e_1 + w^2 e_2 + w^3 e_3).
$$
Note that since $\omega$ is alternating, for any $i, j$ we have
\begin{align*}
  \omega(e_i, e_i) &= 0 \\
  \omega(e_i, e_j) &= -\omega(e_i, e_j)
\end{align*}
so that since $\omega$ is linear,
\begin{align*}
   \omega
&= (v^1 w^2 - v^2 w^1)\omega(e_1, e_2) \\
&+ (v^1 w^3 - v^3 w^1)\omega(e_1, e_3) \\
&+ (v^2 w^3 - v^3 w^2)\omega(e_2, e_3)
\end{align*}
\end{xmpl}
\end{remark}

\begin{prop}
$\{ \alpha^I | |I| = k, I \text{ increasing}\}$ forms a basis for $A_k(V)$.
\end{prop}
\begin{corol}
$\dim_{\mathbb{R}} A_k(V) = {n \choose k}$ (since there are this many
increasing subsequences of length $k$ in $\{1, \dots, n\}$.
\end{corol}

\begin{proof}
Assume $c_I \in \mathbb{R}$ and see that
\begin{align*}
   0
&= \left(\sum_{|I|=k} c_I \alpha^I\right)(e_J) \\
&= \sum c_I \alpha^I(e_J) \\
&= \sum c_I \delta^I_J = c_J
\end{align*}
for any $J$. Hence the $\alpha$ are linearly independent.

Let $\omega \in A_k(V)$, and define
\begin{align*}
     \alpha
&=   \sum_I \omega(e_I) \alpha^I \\
&\in \mathrm{span}(\{ \alpha^I \}).
\end{align*}
We see that $\alpha = \omega$ because of the remark above and because
for $J$ increasing
$$
  \alpha(e_J)
= \sum_I \omega(e_I) \alpha^I(e_J)
= \omega(e_J).
$$
\end{proof}

In particular $\dim(A_n(V)) = {n \choose n} = 1$, and $A_n(V)$ is
spanned by $\det: V \times \cdots \times V \to \mathbb{R}$.
For $k > n$, $A_k(V) = \{ 0 \}$.

\section*{Differential forms}

\begin{defn}[Differential forms]
Let $U \subset \mathbb{R}^n$ be open. A \emph{differential $k$-form}
or \emph{$k$-form} on $U$ is a function
$\omega : U \to \sqcup_{p \in U} A_k(T_p \mathbb{R}^n)$ given by
$p \mapsto \omega_p \in A_k(T_p \mathbb{R}^n)$.
\end{defn}

\begin{xmpl}

  \begin{enumerate}
    \item{
       For $k = 0$,
       $A_0(T_p \mathbb{R}^n) = \mathbb{R}$, so a $0$-form is simply a
       function $\omega: U \to \mathbb{R}$.
    }
    \item{
      For $f \in C^\infty(U)$, $\dif f$ is a $1$-form defined by
      $(\dif f)_p : T_p \mathbb{R}^n \to \mathbb{R}$ given by
      $(\dif f)_p (X_p) \triangleq X_p(f)$. Recall also that
      $T_p \mathbb{R}^n \simeq \mathcal{D}_p \mathbb{R}^n$.
    }
  \end{enumerate}
\end{xmpl}

On $\mathbb{R}^n$ we have standard coordinates $(x^i)$, which can be
thought of as coordinate functions $x^i: U \to \mathbb{R}$,
i.e. projections from the tuple $(p^1, \dots, p^n)$ to $p^i$, and
$x^i \in C^\infty(U)$. For $p \in U$,
$T_p \mathbb{R}^n = \mathcal{D}_p \mathbb{R}^n$ has a basis
$$
\left\{ \left.\frac{\partial}{\partial x^1}\right|_p,
         \dots,
         \left.\frac{\partial}{\partial x^n}\right|_p
\right\}.
$$
\begin{lemma}
$\{\dif x_p^i\} \subset A_1(T_p \mathbb{R}^n)$ is a dual basis to this basis.
\end{lemma}
\begin{proof}
\begin{align*}
  \dif x_p^i
    \left(
      \left.\frac{\partial}{\partial x^j}\right|_p
    \right)
&\triangleq
  \left.\frac{\partial}{\partial x^j}\right|_p(x^i) \\
&= \frac{\partial x^i}{\partial x^j}(p)
 = \delta_j^i.
\end{align*}
\end{proof}

\begin{corol}
The collection of all
$\dif x_p^I = \dif x_p^{i_1} \wedge \cdots \wedge \dif x_p^{i_k}$
forms a basis for $A_k(T_p \mathbb{R}^n)$
\end{corol}

\begin{defn}[Smooth forms]
Given a $k$-form $\omega$ on $U$,
\begin{align*}
   \omega_p
&= \sum_I a_i(p) \dif x_p^I,
\end{align*}
and we may write $\omega = \sum_I a_i \dif x^I$ where
$a_i: U \to \mathbb{R}$. We say that the form $\omega$ is smooth when
each coordinate function $a_i \in C^\infty(U)$. The space of all
smooth $k$-forms is denoted $\Omega^k(U)$, which has induced addition
and scalar multiplication and is therefore a real vector space as well as a
module over the smooth functions $U \to \mathbb{R}$.
\end{defn}

\begin{prop}
Recall that for $f \in C^\infty(U)$,
$(\dif f)_p (X_p) \triangleq X_p(f)$. In coordinates,
$$
  \dif f
= \sum_{i=1}^n \frac{\partial f}{\partial x^i} \dif x^i.
$$
\end{prop}
\begin{proof}
Since the $\dif x^i$ form a basis for
$A_1(T_p \mathbb{R}^n)$, we can write
$$
  \dif f
= \sum_{i=1}^n a_i \dif x^i
$$
for some $a_i : U \to \mathbb{R}$, and
\begin{align*}
   \frac{\partial}{\partial x^j}(f)
&\triangleq
   \dif f \left(
     \frac{\partial}{\partial x^j}
   \right) \\
&= \left(
     \sum_{i=1}^n a_i \dif x^i
   \right)
   \left(
     \frac{\partial}{\partial x^j}
   \right) \\
&= \sum_i
     a_i \dif x^i
     \left(
       \frac{\partial}{\partial x^j}
     \right) \\
&= \sum_i a_i \delta_j^i = a_j.
\end{align*}
\end{proof}
As a corollary, we have a function
$\dif : C^\infty(U) \to \Omega^i(U)$ and indeed
$d: \Omega^k(U) \to \Omega^{k+1}(U)$.


The wedge product
$\omega \wedge \tau = \frac{1}{k! l!}A(\omega \otimes \tau)$
induces a wedge product on differential forms
$\wedge : \Omega^k(U) \times \Omega^l(U) \to \Omega^{k+l}(U)$
by
$$
(\omega \wedge \tau)_p \triangleq \omega_p \wedge \tau_p
$$
since $\omega_p \wedge \tau_p \in A_{k+l}(T_p \mathbb{R}^n)$.

The wedge product of differential forms is
\begin{enumerate}
  \item{
    $C^\infty(U)$-bilinear, i.e.
    $$
      (f_1 \omega_1 + f_2 \omega_2) \wedge \tau
    = f_1(\omega_1 \wedge \tau) + f_2(\omega_2 \wedge \tau)
    $$
    where $f_1, f_2 \in C^\infty(U)$,
  }
  \item{
    graded commutative, i.e.
    $\omega \wedge \tau = (-1)^{kl} \tau \wedge \omega$
  }
  \item{
    associative.
  }
\end{enumerate}

\subsection{Exterior Derivative}
\begin{defn}[Exterior derivative]
The \emph{exterior derivative} $\dif = \dif_k$ is a map
$\Omega^k(U) \to \Omega^{k+1}(U)$ given by
\begin{align*}
   \dif \omega
&= \left(
     \dif \sum_{|I| = k} a_I \dif x^I
   \right) \\
&= \sum_{|I| = k} \dif (a_I) \wedge \dif x^I \\
&= \sum_I \sum_i
     \frac{\partial (a_I)}
          {\partial x^i}
     \dif x^i \wedge \dif x^I.
\end{align*}
\end{defn}

\begin{remark}[Properties]
  \begin{enumerate}
    \item{
      $\Omega^0(U) \to \Omega^1(U)$ satisfies
      $(\dif f)_p(X_p) = X_p(f)$.
    }
    \item{
      $\dif^2 = 0$, i.e.
      $d \circ d : \Omega^k(U) \to \Omega^{k+2}(U)$ is the zero map.
    }
    \item{
      $
        \dif (\omega \wedge \tau)
      = \dif \omega \wedge \tau
      + (-1)^{\deg(\omega)} \omega \wedge \dif \tau.
      $
    }
  \end{enumerate}
  This is called the \emph{deRham cochain complex}, and we will later
  study its cohomology.
\end{remark}

\begin{prop}
If $D_k: \Omega^k(U) \to \Omega^{k+1}(U)$ is a sequence of
$\mathbb{R}$-linear maps satisfying the properties above, then
$D_k = \dif_k$.
\end{prop}
\begin{proof}
By assumption, $D_0 = \dif_0$ since this is how $\dif_0$ is defined.

We claim that for all increasing $I$, $|I| = k$,
$D(\dif x^I) = 0$. For $k = 1$,
$$
D(\dif x^i) = D(D x^i) = 0
$$
by assumption of property 2.

Let $|I| = k + 1$. Then
\begin{align*}
   D(\dif x^I)
&= D(\dif x^{i_1} \wedge \cdots \wedge \dif x^{i_{k+1}}) \\
&= D(\dif x^{i_1} \wedge \dif x^J) \\
&= D(\dif x^{i_1}) \wedge \dif x^J
 + (-1)^1 \dif x^{i_1} \wedge D(\dif x^J) \\
&= 0.
\end{align*}
By linearity, to show $D = \dif$ it suffices to check that
$D(f \dif x^I) = \dif (f \dif x^I)$ for any
$f \dif x^I \in \Omega^k(U)$.
We have
\begin{align*}
   D(f \dif x^I)
&= D(f \wedge \dif x^I) \\
&= D(f) \wedge \dif x^I + (-1)^0 f \wedge D(\dif x^I) \\
&= D(f) \wedge \dif x^I \\
&= \dif f \wedge \dif x^I \\
&= \dif(f \dif x^I).
\end{align*}
Therefore the exterior derivative is unique among operators with these
properties.
\end{proof}

\begin{proof}[Properties of the exterior derivative]
  \begin{enumerate}
    \item{
      We check that
      $\dif^2 (f \dif x^I) = 0$.
      \begin{align*}
         \dif^2 (f \dif x^I)
      &= \dif (\dif f \wedge \dif x^I) \\
      &= \dif \left(
           \sum_i
             \frac{\partial f}{\partial x^i}
             \dif x^i \wedge \dif x^I
         \right) \\
      &= \sum_i
           \dif \left(
             \frac{\partial f}
                  {\partial x^i}
           \right)
           \dif x^i \wedge \dif x^I \\
      &= \sum_i
           \sum_j
             \frac{\partial^2 f}
                  {\partial x^j \partial x^i}
             \dif x^j \wedge \dif x^i \wedge \dif x^I.
      \end{align*}

      Note that any term with $i = j$ is zero since
      $\dif x^i \wedge \dif x^i = 0$. Summands with $i \neq j$
      appear twice, with $i, j$ interchanged, i.e.
      $$
        \frac{\partial^2 f}
             {\partial x^i \partial x^j}
           \dif x^i \wedge \dif x^j \wedge \dif x^I
      + \frac{\partial^2 f}
             {\partial x^j \partial x^i}
           \dif x^j \wedge \dif x^i \wedge \dif x^I
      = \frac{\partial^2 f}
             {\partial x^i \partial x^j}
           \dif x^i \wedge \dif x^j \wedge \dif x^I
      - \frac{\partial^2 f}
             {\partial x^j \partial x^i}
           \dif x^i \wedge \dif x^j \wedge \dif x^I
      = 0.
      $$
    }
    \item{
      We next check that
      \begin{align*}
         \dif (\omega \wedge \tau)
      &= \dif (f g \dif x^I \wedge \dif x^J) \\
      &= \dif (f g) \wedge \dif x^I \wedge \dif x^J \\
      &= \sum_i
            \frac{\partial (fg)}
                 {\partial x^i}
            \dif x^i \wedge \dif x^I \wedge \dif x^J \\
      &= \sum_i
            \left(
              \frac{\partial f}
                   {\partial x^i}
            \right)
            g
            \dif x^i \wedge \dif x^I \wedge \dif x^J
       + \sum_i
            \left(
              \frac{\partial g}
                   {\partial x^i}
            \right)
            f
            \dif x^i \wedge \dif x^I \wedge \dif x^J \\
      &= \sum_i
           \left(
             \frac{\partial f}
                  {\partial x^i}
             \dif x^i \wedge \dif x^I
           \right)
           \wedge (g \dif x^J) \\
       + (-1)^{1 \cdot |I|}
           \sum_i
             f \frac{\partial g}{\partial x^i}
             \dif x^I \wedge \dif x^i \wedge \dif x^J \\
      &= \dif (f \dif x^I) \wedge (g \dif x^J)
      &+ (-1)^{\deg(\omega)} (f \dif x^I) \wedge \dif(g \dif x^J) \\
      &= \dif (\omega) \wedge \tau
       + (-1)^{\deg(\omega)} \omega \wedge \dif \tau.
      \end{align*}
    }
  \end{enumerate}
\end{proof}

\subsection{deRham cohomology}
\begin{defn}[Closed and exact form]
A $k$-form $\omega$ is \emph{closed} if
$\dif \omega = 0$, i.e. if it is in the kernel of
$\dif$, and \emph{exact} if
$\omega = \dif \alpha$ for some $\alpha$, i.e. if it is in the image
of $\dif$.
\end{defn}
Note that exact $k$-forms are closed since $\dif^2 = 0$.

\begin{defn}[deRham cohomology]
The \emph{$k$-th deRham cohomology group} of $U$ is the quotient group
$H^k(U) \triangleq \ker(\dif_k) / \mathrm{Im}(\dif_{k-1})$. We augment
the chain by defining $\dif_{-1}$ to be the inclusion map from $\{ 0 \}$.
\end{defn}

\begin{remark}
The $k$-th deRham cohomology gives a sense of the $k$-dimensional
holes in $U$ from the perspective of calculus. We will see that, for instance,
$$
H^0(U) \simeq \mathbb{R}^{\text{\# of connected components of U}}.
$$
\end{remark}

\begin{xmpl}
$$
  H^0(U)
= \ker(d_0) / \ker(d_{-1})
= \ker(d_0) / \{ 0 \}
= \ker(d_0).
$$
In general $f \in \ker \dif_0$ means that
\begin{align*}
   \dif f
&= \sum_i
     \frac{\partial f}{\partial x^i}
     \dif x^i = 0
\end{align*}
which is only true if all partial derivatives
$\frac{\partial f}{\partial x^i} = 0$, i.e. $f$ is locally constant.

If we have a connected component of $U$, then we may connect any two
points in this space by a path $c(t)$, and
$$
  f(c(1)) - f(c(0))
= \int_0^1 \frac{\dif}{\dif t} f(c(t))
= \int_0^1
    \sum_i
      \frac{\partial f}
           {\partial x^i}
      (c(t)) c_i^\prime(t)
      \dif t
= 0.
$$
\end{xmpl}

We have a cochain given by $\dif^k$:
$$
    \{ 0 \}
\to \Omega^0(U) = C^\infty(U)
\to \cdots
\to \Omega^n(U)
\to \{ 0 \}
$$
with $d^2 = 0$,
so $\mathrm{Im}(\dif^{k-1}) < \ker(\dif^k)$. Then we can define the
deRham cohomology $H^k(U) = \ker(\dif^k) / \mathrm{Im}(\dif^{k-1})$.
Elements $[\omega] \in H^k(U)$ are cosets
$\omega + \dif^{k-1}(\Omega^{k-1}(U))$, where $\dif^k(\omega) = 0$,
i.e. $[\omega_1] = [\omega_2] \in H^k(U)$ if and only if
$\omega_1 - \omega_2 = \dif^{k-1} \alpha$ for some
$\alpha \in \Omega^{k-1}(U)$. We have seen that
$H^0(U)$ is isomorphic to the $\mathbb{R}$-vector space of locally
constant functions on $U$, i.e. those whose derivative is everywhere
0. Indeed, this space is $\mathbb{R}^N$, where $N$ is the number of
connected components of $U$.

\begin{lemma}[Poincar\'e Lemma]
If $U \subset \mathbb{R}^n$ is star shaped, then $H^1(U) = \{ 0 \}$.
\end{lemma}
\begin{proof}
Let $[\omega] \in H^1(U)$. Then $\omega \in \Omega^1(U)$ satisfies
$\dif \omega = 0$. In coordinates,
\begin{align*}
   0
&= \dif \omega \\
&= \dif \left(\sum_i f_i \dif x^i\right) \\
&= \sum_{i < j}
     \left(
       \frac{\partial f_i}{\partial x^j}
     - \frac{\partial f_j}{\partial x^i}
    \right)
    \dif x^i \wedge \dif x^j
\end{align*}
so that $\frac{\partial f_i}{\partial x^j} = \frac{\partial
  f_j}{\partial x^i}$.
We want to show that there exists some
$f \in \Omega^0(U)$ such that
$\dif f = \omega - 0 = \omega$.

For simplicity, assume $U$ is star-shaped with respect to 0.
Let $x \in U$, so that the line segment from 0 to $x$ can be
parameterized
$$
  c(t)
= (t x^1, \dots, t x^n).
$$
Define
$$
  f(x^1, \dots, x^n)
= \int_0^1
    \sum_i
      x^i f_i(tx^1, \dots, tx^n)
      \dif t.
$$
Then
\begin{align*}
   \frac{\partial f}
        {\partial x^j}
   (x^1, \dots, x^n)
&= \int_0^1
     \sum_i
       \left(
         \frac{\partial x^i}{\partial x^j}
           f_i(tx^1, \dots, t x^n)
       + x^i
         \frac{\partial}{\partial x^j}
           f_i(tx^1, \dots, tx^n)
       \right) \dif t\\
&= \int_0^1
     \left[
       \sum_i
         \delta^i_j
         f_i(tx^1, \dots, tx^n)
     + \sum_i
         t x^i
         \left.
           \frac{\partial f_i}{\partial x^j}
         \right|_{(tx^1, \dots, tx^n)}
     \right] \dif t \\
&= \int_0^1
     \left[
       f_j(t x^1, \dots, tx^n)
     + t
       \left.
         \frac{\dif f_i}{\dif t}
       \right|_{(tx^1, \dots, tx^n)}
     \right] \dif t \\
&= \int_0^1
     \frac{\dif}{\dif t}
     (t f_j(tx^1, \dots, t x^n))
     \dif t \\
&= f_j(x^1, \dots, x^n)
\end{align*}
using the condition found above on the partial derivatives.

This choice of integrand is motivated by the fact that if $\dif f =
\omega$ then
$$
  \frac{\partial f}{\partial x^i}
= f_i
= \int_0^1 \frac{\dif}{\dif t}(f \circ c)
= f(x) - f(0).
$$
\end{proof}

\begin{xmpl}
$H^1(\mathbb{R}^2 \setminus \{ 0 \}) \neq \{ 0 \}$.
We can show this by finding a 1-form
$\omega \in \Omega^1(\mathbb{R}^2 \setminus \{ 0 \})$
with $\dif \omega = 0$ and $\omega \neq \dif f$ for any
$f: \mathbb{R}^2 \setminus \{ 0 \} \to \mathbb{R}$. We choose
$$
  \omega
= \frac{-y}{x^2 + y^2} \dif x
+ \frac{x}{x^2 + y^2} \dif y,
$$
which would satisfy $\dif \left(\arctan \frac{y}{x}\right) = \omega$,
but this function is not defined on $\mathbb{R}^2 \setminus \{ 0 \}$.
We can check that $\dif \omega = 0$ easily.

Suppose $\omega = \dif g$ for some $g$. Then we would have
\begin{align*}
   \frac{\dif}{\dif t}
     f(\cos(t), \sin(t))
&= \nabla f(\cos(t), \sin(t)) \cdot (-\sin t, \cos t) \\
&= (-\sin t, \cos t) \cdot (-\sin t, \cos t) \\
&= 1
\end{align*}
so that
\begin{align*}
   2\pi
&= \int_{t=0}^{2\pi}
     1 \dif t \\
&= \int_{0}^{2\pi}
     \frac{\dif}{\dif t}
       f(c(t)) \dif t \\
&= f(c(2\pi)) - f(c(0)) = 0,
\end{align*}
a contradiction.
\end{xmpl}

\begin{xmpl}
In $\mathbb{R}^3$ we have
$$
\{0\}                     \rightarrow
C^\infty(U)               \xrightarrow{\mathrm{grad}}
C^\infty(U, \mathbb{R}^3) \xrightarrow{\mathrm{curl}}
C^\infty(U, \mathbb{R}^3) \xrightarrow{\mathrm{div}}
C^\infty(U)               \rightarrow
\{0\}
$$
which has isomorphisms corresponding to
$$
\{0\}       \rightarrow
\Omega^0(U) \xrightarrow{\dif}
\Omega^1(U) \xrightarrow{\dif}
\Omega^2(U) \xrightarrow{\dif}
\Omega^3(U) \rightarrow
\{0\},
$$
respectively given by
\begin{align*}
& \mathrm{Id}, \\
& \langle P, Q, R \rangle
    \mapsto P \dif x + Q \dif y + R \dif z, \\
& \langle P, Q, R \rangle
    \mapsto P \dif y \wedge \dif z
          + Q \dif z \wedge \dif x
          + R \dif x \wedge \dif y, \\
& f \mapsto f \dif x \wedge \dif y \wedge \dif z.
\end{align*}
\end{xmpl}
